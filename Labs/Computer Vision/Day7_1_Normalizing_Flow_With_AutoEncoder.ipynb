{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image.png](https://i.imgur.com/a3uAqnb.png)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "In this notebook, we will create a RealNVP(real-valued non-volume preserving) generative model for the MNIST dataset, with an intermediate AutoEncoder.\n",
    "\n",
    "Instead of training the model on direct pixel values and generating images, we will\n",
    "\n",
    "1. Train an AutoEncoder for the images\n",
    "2. Convert the data into embeddings using the AutoEncoder\n",
    "3. Train our RealNVP model on the embeddings\n",
    "4. Generate embeddings using the RealNVP model and convert them to images using the AutoEncoder's decoder\n",
    "\n",
    "This notebook is heavily based on [This Repo](https://github.com/SpencerSzabados/realnvp-pytorch/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, distributions\n",
    "from torch.nn import BCELoss\n",
    "from torchvision import datasets, transforms"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LinearBatchNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    An (invertible) batch normalization layer.\n",
    "    This class is mostly inspired from this one:\n",
    "    https://github.com/kamenbliznashki/normalizing_flows/blob/master/maf.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, momentum=0.9, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        self.log_gamma = nn.Parameter(torch.zeros(input_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(input_size))\n",
    "\n",
    "        self.register_buffer('running_mean', torch.zeros(input_size))\n",
    "        self.register_buffer('running_var', torch.ones(input_size))\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.training:\n",
    "            self.batch_mean = x.mean(0)\n",
    "            self.batch_var = x.var(0)\n",
    "\n",
    "            self.running_mean.mul_(self.momentum).add_(self.batch_mean.data * (1 - self.momentum))\n",
    "            self.running_var.mul_(self.momentum).add_(self.batch_var.data * (1 - self.momentum))\n",
    "\n",
    "            mean = self.batch_mean\n",
    "            var = self.batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        y = self.log_gamma.exp() * x_hat + self.beta\n",
    "\n",
    "        log_det = self.log_gamma - 0.5 * torch.log(var + self.eps)\n",
    "\n",
    "        return y, log_det.expand_as(x).sum(1)\n",
    "\n",
    "    def backward(self, x, **kwargs):\n",
    "        if self.training:\n",
    "            mean = self.batch_mean\n",
    "            var = self.batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        x_hat = (x - self.beta) * torch.exp(-self.log_gamma)\n",
    "        x = x_hat * torch.sqrt(var + self.eps) + mean\n",
    "\n",
    "        log_det = 0.5 * torch.log(var + self.eps) - self.log_gamma\n",
    "\n",
    "        return x, log_det.expand_as(x).sum(1)\n",
    "\n",
    "\n",
    "class LinearCouplingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear coupling layer.\n",
    "        (i) Split the input x into 2 parts x1 and x2 according to a given mask.\n",
    "        (ii) Compute s(x2) and t(x2) with given neural network.\n",
    "        (iii) Final output is [exp(s(x2))*x1 + t(x2); x2].\n",
    "    The inverse is trivially [(x1 - t(x2))*exp(-s(x2)); x2].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, mask, network_topology, conditioning_size=None, single_function=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if conditioning_size is None:\n",
    "            conditioning_size = 0\n",
    "\n",
    "        if network_topology is None or len(network_topology) == 0:\n",
    "            network_topology = [input_dim]\n",
    "\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "        self.dim = input_dim\n",
    "\n",
    "        self.s = [nn.Linear(input_dim + conditioning_size, network_topology[0]), nn.ReLU()]\n",
    "\n",
    "        for i in range(len(network_topology)):\n",
    "            t = network_topology[i]\n",
    "            t_p = network_topology[i - 1]\n",
    "            self.s.extend([nn.Linear(t_p, t), nn.ReLU()])\n",
    "\n",
    "        if single_function:\n",
    "            input_dim = input_dim * 2\n",
    "\n",
    "        ll = nn.Linear(network_topology[-1], input_dim)\n",
    "\n",
    "        self.s.append(ll)\n",
    "        self.s = nn.Sequential(*self.s)\n",
    "\n",
    "        if single_function:\n",
    "            self.st = lambda x: (self.s(x).chunk(2, 1))\n",
    "        else:\n",
    "            self.t = copy.deepcopy(self.s)\n",
    "            self.st = lambda x: (self.s(x), self.t(x))\n",
    "\n",
    "    def backward(self, x, y=None):\n",
    "        mx = x * self.mask\n",
    "\n",
    "        if y is not None:\n",
    "            _mx = torch.cat([y, mx], dim=1)\n",
    "        else:\n",
    "            _mx = mx\n",
    "\n",
    "        s, t = self.st(_mx)\n",
    "        s = torch.tanh(s)\n",
    "\n",
    "        u = mx + (1 - self.mask) * (x - t) * torch.exp(-s)\n",
    "\n",
    "        log_abs_det_jacobian = - (1 - self.mask) * s\n",
    "\n",
    "        return u, log_abs_det_jacobian.sum(1)\n",
    "\n",
    "    def forward(self, u, y=None):\n",
    "        mu = u * self.mask\n",
    "\n",
    "        if y is not None:\n",
    "            _mu = torch.cat([y, mu], dim=1)\n",
    "        else:\n",
    "            _mu = mu\n",
    "\n",
    "        s, t = self.st(_mu)\n",
    "        s = torch.tanh(s)\n",
    "\n",
    "        x = mu + (1 - self.mask) * (u * s.exp() + t)\n",
    "\n",
    "        log_abs_det_jacobian = (1 - self.mask) * s\n",
    "\n",
    "        return x, log_abs_det_jacobian.sum(1)\n",
    "\n",
    "\n",
    "class Permutation(nn.Module):\n",
    "    \"\"\"\n",
    "    A permutation layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.register_buffer('p', torch.randperm(in_ch))\n",
    "        self.register_buffer('invp', torch.argsort(self.p))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        assert x.shape[1] == self.in_ch\n",
    "        out = x[:, self.p]\n",
    "        return out, 0\n",
    "\n",
    "    def backward(self, x, y=None):\n",
    "        assert x.shape[1] == self.in_ch\n",
    "        out = x[:, self.invp]\n",
    "        return out, 0\n",
    "\n",
    "\n",
    "class SequentialFlow(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Utility class to build a normalizing flow from a sequence of base transformations.\n",
    "    During forward and inverse steps, aggregates the sum of the log determinants of the Jacobians.\n",
    "    \"\"\"\n",
    "    def forward(self, x, y=None):\n",
    "        log_det = 0\n",
    "        for module in self:\n",
    "            x, _log_det = module(x, y=y)\n",
    "            log_det = log_det + _log_det\n",
    "        return x, log_det\n",
    "\n",
    "    def backward(self, u, y=None):\n",
    "        log_det = 0\n",
    "        for module in reversed(self):\n",
    "            u, _log_det = module.backward(u, y=y)\n",
    "            log_det = log_det + _log_det\n",
    "        return u, log_det\n",
    "\n",
    "    def forward_steps(self, x, y=None):\n",
    "        log_det = 0\n",
    "        xs = [x]\n",
    "        for module in self:\n",
    "            x, _log_det = module(x, y=y)\n",
    "            xs.append(x)\n",
    "            log_det = log_det + _log_det\n",
    "        return xs, log_det\n",
    "\n",
    "    def backward_steps(self, u, y=None):\n",
    "        log_det = 0\n",
    "        us = [u]\n",
    "        for module in reversed(self):\n",
    "            u, _log_det = module.backward(u, y=y)\n",
    "            us.append(u)\n",
    "            log_det = log_det + _log_det\n",
    "        return us, log_det\n",
    "\n",
    "\n",
    "class LinearRNVP(nn.Module):\n",
    "    \"\"\"\n",
    "    Main RNVP model, alternating affine coupling layers\n",
    "    with permutations and/or batch normalization steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, coupling_topology, flow_n=2, use_permutation=False,\n",
    "                 batch_norm=False, mask_type='odds', conditioning_size=None, single_function=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer('prior_mean', torch.zeros(input_dim))\n",
    "        self.register_buffer('prior_var', torch.ones(input_dim))\n",
    "\n",
    "        if mask_type == 'odds':\n",
    "            mask = torch.arange(0, input_dim).float() % 2\n",
    "        elif mask_type == 'half':\n",
    "            mask = torch.zeros(input_dim)\n",
    "            mask[:input_dim // 2] = 1\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        if coupling_topology is None:\n",
    "            coupling_topology = [input_dim // 2, input_dim // 2]\n",
    "\n",
    "        blocks = []\n",
    "\n",
    "        for i in range(flow_n):\n",
    "\n",
    "            blocks.append(LinearCouplingLayer(input_dim, mask, network_topology=coupling_topology,\n",
    "                                              conditioning_size=conditioning_size, single_function=single_function))\n",
    "\n",
    "\n",
    "            if use_permutation:\n",
    "                blocks.append(Permutation(input_dim))\n",
    "            else:\n",
    "                mask = 1 - mask\n",
    "\n",
    "            if batch_norm:\n",
    "                blocks.append(LinearBatchNorm(input_dim))\n",
    "\n",
    "        self.flows = SequentialFlow(*blocks)\n",
    "\n",
    "    def logprob(self, x):\n",
    "        return self.prior.log_prob(x)\n",
    "\n",
    "    @property\n",
    "    def prior(self):\n",
    "        return distributions.Normal(self.prior_mean, self.prior_var)\n",
    "\n",
    "    def forward(self, x, y=None, return_step=False):\n",
    "        if return_step:\n",
    "            return self.flows.forward_steps(x, y)\n",
    "        return self.flows.forward(x, y)\n",
    "\n",
    "    def backward(self, u, y=None, return_step=False):\n",
    "        if return_step:\n",
    "            return self.flows.backward_steps(u, y)\n",
    "        return self.flows.backward(u, y)\n",
    "\n",
    "    def sample(self, samples=1, y=None, return_step=False, return_logdet=False):\n",
    "        u = self.prior.sample((samples,))\n",
    "        z, d = self.backward(u, y=y, return_step=return_step)\n",
    "        if return_logdet:\n",
    "            d = self.logprob(u).sum(1) + d\n",
    "            return z, d\n",
    "        return z"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set the random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the dataset\n",
    "train_set = datasets.MNIST('./data',\n",
    "                           train=True,\n",
    "                           download=True,\n",
    "                           transform=transform,)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, BATCH_SIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder definition"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **üìå Autoencoder for Dimensionality Reduction**\n",
    "Before we build the Normalizing Flow, we first need a way to represent the high-dimensional MNIST images (28x28 = 784 pixels) in a much smaller, more manageable space. An **Autoencoder** is perfect for this task.\n",
    "\n",
    "## **üîπ How it Works**\n",
    "1Ô∏è‚É£ **Encoder**: A neural network that compresses the input image into a low-dimensional latent vector (embedding). This embedding captures the most important features of the image.\n",
    "\n",
    "2Ô∏è‚É£ **Decoder**: Another neural network that reconstructs the original image from the latent vector.\n",
    "\n",
    "By training the Autoencoder to minimize the difference between the original and reconstructed image, the encoder learns to create meaningful, information-rich embeddings. We will then train our Normalizing Flow model on these embeddings instead of the raw pixel data.\n",
    "\n",
    "\n",
    "\n",
    "## **üìå Expected Input & Output Shapes**\n",
    "- **Input Image:** `(batch_size, 1, 28, 28)`\n",
    "- **Latent Embedding:** `(batch_size, 20)`  *(20 is our `EMBEDDING_DIM`)*\n",
    "- **Reconstructed Image:** `(batch_size, 1, 28, 28)`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "EMBEDDING_DIM = 20      # The dimension of the embeddings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple autoencoder for images. \n",
    "    self.linear1 generates the intermediate embeddings that we use for the normalizing flow.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoding layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, stride=2, kernel_size=3, bias=False, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, stride=2, kernel_size=3, bias=False, padding=1)\n",
    "        self.linear1 = nn.Linear(in_features=3136, out_features=EMBEDDING_DIM)\n",
    "        \n",
    "        # Decoding layers\n",
    "        self.linear2 = nn.Linear(in_features=EMBEDDING_DIM, out_features=3136)\n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels=64, out_channels=32, stride=2, kernel_size=3, padding=1, output_padding=1)\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels=32, out_channels=1, stride=2, kernel_size=3, padding=1, output_padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        emb = self.encoder(x)\n",
    "        _x = self.decoder(emb)\n",
    "        \n",
    "        return _x, emb\n",
    "    \n",
    "    def decoder(self, emb):\n",
    "\n",
    "        _x = torch.relu(self.linear2(emb))\n",
    "        _x = _x.view(-1, 64, 7, 7)\n",
    "        _x = torch.relu(self.convt1(_x))\n",
    "        _x = self.convt2(_x)\n",
    "        \n",
    "        return _x\n",
    "    \n",
    "    def encoder(self, x):\n",
    "        _x = torch.relu(self.conv1(x))\n",
    "        _x = torch.relu(self.conv2(_x))\n",
    "        sh = _x.shape\n",
    "\n",
    "        _x = torch.relu(torch.flatten(_x, 1))\n",
    "        \n",
    "        emb = self.linear1(_x)\n",
    "        \n",
    "        return emb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder training on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build the autoencoder\n",
    "autoencoder = AutoEncoder()\n",
    "autoencoder = autoencoder.to(device)\n",
    "\n",
    "criterion = BCELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "AE_EPOCHS = 10  # Epochs for training the autoencoder"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for j in range(AE_EPOCHS):\n",
    "\n",
    "    losses = []\n",
    "    for batch_idx, data in enumerate(tqdm(train_loader)):\n",
    "\n",
    "        x, _ = data\n",
    "        x = x.to(device)\n",
    "\n",
    "        # Run the autoencoder\n",
    "        _x, emb = autoencoder(x)\n",
    "        _x = torch.sigmoid(_x)\n",
    "\n",
    "        # Compute Reconstruction loss\n",
    "        rec_loss = criterion(_x, x)\n",
    "\n",
    "        losses.append(rec_loss.item())\n",
    "\n",
    "        autoencoder.zero_grad()\n",
    "        rec_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    print(f'Epoch #{j+1}, Loss: {sum(losses)/len(losses):.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new dataset containing the embeddings and the associated labels\n",
    "\n",
    "We replace the original x with the corresponding embedding from the trained autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "embedded_data = []\n",
    "\n",
    "for batch_idx, data in enumerate(tqdm(train_loader)):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x, y = data\n",
    "\n",
    "        x = x.to(device)\n",
    "\n",
    "        _, emb = autoencoder(x)\n",
    "\n",
    "        for j in range(len(emb)):\n",
    "            embedded_data.append((emb[j], y[j]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "embedded_train_loader = torch.utils.data.DataLoader(embedded_data, BATCH_SIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Flow training"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **üìå RealNVP Normalizing Flow Model**\n",
    "A **Normalizing Flow** is a type of generative model that learns an explicit, invertible transformation between a complex data distribution (like our image embeddings) and a simple base distribution (like a standard normal distribution). The model we use here is **RealNVP (Real-valued Non-Volume Preserving)**.\n",
    "\n",
    "## **üîπ Key Concepts**\n",
    "1Ô∏è‚É£ **Invertible Transformation**: The core idea is a function `f` that can map a data point `x` to a latent point `z` (`z = f(x)`), and can also be perfectly inverted to map `z` back to `x` (`x = f‚Åª¬π(z)`). This allows for both density estimation and generation.\n",
    "\n",
    "2Ô∏è‚É£ **Change of Variables Formula**: Normalizing Flows use this formula to calculate the exact likelihood of a data point. The loss function aims to maximize this likelihood. The key is that the transformation's Jacobian determinant must be easy to compute.\n",
    "\n",
    "3Ô∏è‚É£ **Coupling Layers**: RealNVP achieves an easily computable Jacobian by using special \"coupling layers.\" These layers split the input vector into two parts, transforming one part based on the other, which is left unchanged. By stacking and alternating these layers, the model can learn highly complex transformations.\n",
    "\n",
    "\n",
    "\n",
    "## **üìå Expected Input & Output Shapes**\n",
    "- **Input (Embeddings):** `(batch_size, 20)`\n",
    "- **Conditional Input (Labels):** `(batch_size, 10)`\n",
    "- **Output (Latent `u`):** `(batch_size, 20)`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "FLOW_N = 9              # Number of affine coupling layers\n",
    "RNVP_TOPOLOGY = [200]   # Size of the hidden layers in each coupling layer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# See the file realmvp.py for the full definition\n",
    "nf_model = LinearRNVP(input_dim=EMBEDDING_DIM, coupling_topology=RNVP_TOPOLOGY,\n",
    "                      flow_n=FLOW_N, batch_norm=True,\n",
    "                      mask_type='odds', conditioning_size=10,\n",
    "                      use_permutation=True, single_function=True).to(device)\n",
    "\n",
    "nf_optimizer = torch.optim.Adam(nf_model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "NF_EPOCHS = 20          # Epochs for training the normalizing flow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for j in range(NF_EPOCHS):\n",
    "\n",
    "    nf_model.train()\n",
    "\n",
    "    losses = []\n",
    "    for batch_idx, data in enumerate(tqdm(embedded_train_loader)):\n",
    "\n",
    "        emb, y = data\n",
    "        emb = emb.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y = nn.functional.one_hot(y, 10).to(device).float()\n",
    "        \n",
    "        # Get the inverse transformation and the corresponding log determinant of the Jacobian\n",
    "        u, log_det = nf_model.forward(emb, y=y) \n",
    "\n",
    "        # Train via maximum likelihood\n",
    "        prior_logprob = nf_model.logprob(u)\n",
    "        log_prob = -torch.mean(prior_logprob.sum(1) + log_det)\n",
    "\n",
    "        losses.append(log_prob.item())\n",
    "\n",
    "        nf_model.zero_grad()\n",
    "        log_prob.backward()\n",
    "        nf_optimizer.step()\n",
    "    \n",
    "    print(f'Epoch #{j+1}, Loss: {sum(losses)/len(losses):.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will train the Normalizing Flow model to learn the distribution of the embeddings created by our Autoencoder.\n",
    "\n",
    "1Ô∏è‚É£ **Forward Pass** ‚Üí Transform an image embedding `emb` into a latent vector `u` from the prior distribution.\n",
    "\n",
    "2Ô∏è‚É£ **Compute Loss** ‚Üí Maximize the log-likelihood using the `log_det` from the transformation and the `logprob` of `u` under the prior.\n",
    "\n",
    "3Ô∏è‚É£ **Backward Pass** ‚Üí Update model parameters to improve the transformation."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_n = 10\n",
    "f, axs = plt.subplots(nrows=10, ncols=sample_n, figsize=(20, 20))\n",
    "\n",
    "for ax in axs:\n",
    "    for a in ax:\n",
    "        a.set_xticklabels([])\n",
    "        a.set_yticklabels([])\n",
    "        a.set_aspect('equal')\n",
    "\n",
    "f.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "nf_model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        y = torch.nn.functional.one_hot(torch.tensor([i]*sample_n), 10).to(device).float()\n",
    "        emb, d = nf_model.sample(sample_n, y=y, return_logdet=True)  # generating the embeddings\n",
    "\n",
    "        # convert embedding to image using autoencoder's decoder\n",
    "        z = autoencoder.decoder(emb)\n",
    "\n",
    "        d_sorted = d.sort(0)[1].flip(0)\n",
    "        z = z[d_sorted]\n",
    "        z = torch.sigmoid(z).cpu()\n",
    "\n",
    "        for j in range(sample_n):\n",
    "            axs[i][j].imshow(z[j].reshape(28, 28), cmap='gray')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üîπ Exercise: Experiment with Hyperparameters**\n",
    "\n",
    "The quality of the generated images depends on both the Autoencoder's representation and the Normalizing Flow's ability to model it. Try experimenting to see how the results change!\n",
    "\n",
    "### **üìù Tasks**\n",
    "\n",
    "1.  **Embedding Dimension**: In the Autoencoder, change `EMBEDDING_DIM`. How does a smaller dimension (e.g., `10`) or a larger one (e.g., `50`) affect the final generated images?\n",
    "2.  **Flow Depth**: Modify `FLOW_N`, the number of coupling layers in the `LinearRNVP` model. Does a deeper flow (e.g., `12`) produce sharper or more diverse samples? What about a shallower one (e.g., `4`)?\n",
    "3.  **Network Topology**: Adjust `RNVP_TOPOLOGY`, which controls the size of the hidden networks inside each coupling layer. Try `[100]` or `[400, 400]`. How does the complexity of these networks impact performance and training time?\n",
    "4.  **Batch Normalization**: Turn off `batch_norm` in the `LinearRNVP` constructor. How does this affect training stability and the final results?\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Contributed by: Ali Habibullah."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
