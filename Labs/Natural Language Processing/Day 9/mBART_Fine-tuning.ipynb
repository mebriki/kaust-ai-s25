{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04f80940689c1ca",
   "metadata": {},
   "source": [
    "![image.png](https://i.imgur.com/a3uAqnb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e0fba964379bbd",
   "metadata": {},
   "source": [
    "# Bidirectional Arabic-English Translation using mBART\n",
    "- **Model**: Facebook's mBART-large-50-one-to-many-mmt pre-trained transformer\n",
    "- **Languages**: Arabic (ar_AR) â†” English (en_XX) bidirectional translation\n",
    "- **Dataset**: Classic Arabic-English Language Pairs from HuggingFace\n",
    "- **Approach**: Fine-tuning with data augmentation (random language swapping)\n",
    "\n",
    "![mBART Architecture](https://i.imgur.com/n8WXfHG.png)\n",
    "\n",
    "## **ğŸ”¹ What is mBART?**\n",
    "**mBART** (Multilingual Bidirectional Auto-Regressive Transformer) is a sequence-to-sequence model pre-trained on 50+ languages. It's particularly effective for:\n",
    "- **Machine Translation** between multiple language pairs\n",
    "- **Cross-lingual understanding** using shared representations\n",
    "- **Few-shot learning** on new language pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b46d6cd2f9ed4",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Package Installation & Setup\n",
    "Installing required packages for transformer fine-tuning and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7450c6b8757eb7f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T02:53:12.452261Z",
     "start_time": "2025-07-05T02:53:12.449994Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential packages for transformer fine-tuning\n",
    "# %pip install gdown          # For downloading datasets from Google Drive\n",
    "# %pip install evaluate       # For computing BLEU scores\n",
    "# %pip install pandas         # For data manipulation\n",
    "# %pip install sentencepiece  # Required for mBART tokenizer\n",
    "# %pip install accelerate     # For distributed training support\n",
    "# %pip install protobuf==3.20.3  # Specific version to avoid conflicts\n",
    "# %pip install matplotlib     # For plotting training curves\n",
    "# %pip install transformers datasets  # HuggingFace transformers and datasets\n",
    "# %pip install torch torchdata --quiet  # PyTorch framework\n",
    "# %pip install tqdm          # Progress bars for training loops\n",
    "# %pip install scikit-learn  # For train-test splitting\n",
    "\n",
    "clear_output()  # Clear installation output for cleaner notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5bd25f82266d5",
   "metadata": {},
   "source": [
    "## ğŸ”§ Import Required Libraries\n",
    "Loading all necessary libraries for data processing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ee6d3bed22c1ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T02:53:36.375866Z",
     "start_time": "2025-07-05T02:53:30.585948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5fd54bb7b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core libraries\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Data handling and processing\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# HuggingFace transformers\n",
    "from transformers import (\n",
    "    GenerationConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    MBart50Tokenizer\n",
    ")\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Evaluation and visualization\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(7)\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853dd99c89aa59a4",
   "metadata": {},
   "source": [
    "## ğŸ—‚ï¸ Dataset Loading and Exploration\n",
    "\n",
    "We'll use the **Classic Arabic-English Language Pairs** dataset, which contains:\n",
    "- **Arabic sentences** (Classical Arabic text)\n",
    "- **English translations** (Modern English equivalents)\n",
    "- **Domain**: Religious and classical texts\n",
    "\n",
    "### Dataset Structure\n",
    "- **Source**: HuggingFace datasets\n",
    "- **Split**: Books collection with paired sentences\n",
    "- **Format**: Each row contains Arabic and English sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d78ee1a4ef3048b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-05T02:54:01.446197Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Dataset Information:\n",
      "Dataset keys: ['quran', 'hadith', 'books']\n",
      "Number of examples: 13331\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Arabic-English translation dataset\n",
    "# This dataset contains classical Arabic texts with English translations\n",
    "dataset = load_dataset(\"Abdulmohsena/Classic-Arabic-English-Language-Pairs\")\n",
    "\n",
    "print(\"ğŸ“Š Dataset Information:\")\n",
    "print(f\"Dataset keys: {list(dataset.keys())}\")\n",
    "print(f\"Number of examples: {len(dataset['books'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "759a4d5ea8a8ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Dataset Structure:\n",
      "DatasetDict({\n",
      "    quran: Dataset({\n",
      "        features: ['ar', 'en'],\n",
      "        num_rows: 9474\n",
      "    })\n",
      "    hadith: Dataset({\n",
      "        features: ['ar', 'en'],\n",
      "        num_rows: 4107\n",
      "    })\n",
      "    books: Dataset({\n",
      "        features: ['ar', 'en'],\n",
      "        num_rows: 13331\n",
      "    })\n",
      "})\n",
      "\n",
      "ğŸ“ Sample Data:\n",
      "\n",
      "Example 1:\n",
      "English: [Chapter on entering upon kings] [Entering of the nobles] If the one entering is from the nobles and the high class, then it is the right of the king to stand in a place where he is not far from him, nor close to him, and to greet him while standing.\n",
      "Arabic: [Ø¨Ø§Ø¨ ÙÙŠ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙˆÙƒ] [Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø£Ø´Ø±Ø§Ù] Ø¥Ù† ÙƒØ§Ù† Ø§Ù„Ø¯Ø§Ø®Ù„ Ù…Ù† Ø§Ù„Ø£Ø´Ø±Ø§Ù ÙˆØ§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©ØŒ ÙÙ…Ù† Ø­Ù‚ Ø§Ù„Ù…Ù„Ùƒ Ø£Ù† ÙŠÙ‚Ù Ù…Ù†Ù‡ Ø¨Ø§Ù„Ù…ÙˆØ¶Ø¹ Ø§Ù„Ø°ÙŠ Ù„Ø§ ÙŠÙ†Ø£Ù‰ Ø¹Ù†Ù‡ØŒ ÙˆÙ„Ø§ ÙŠÙ‚Ø±Ø¨ Ù…Ù†Ù‡ØŒ ÙˆØ£Ù† ÙŠØ³Ù„Ù… Ø¹Ù„ÙŠÙ‡ Ù‚Ø§Ø¦Ù…Ø§.\n",
      "\n",
      "Example 2:\n",
      "English: If he approached him, he would come close to him, lean on his limbs and kiss them, then move away from him and stand until he stands at a level like his.\n",
      "Arabic: ÙØ¥Ù† Ø§Ø³ØªØ¯Ù†Ø§Ù‡ØŒ Ù‚Ø±Ø¨ Ù…Ù†Ù‡ØŒ ÙØ£ÙƒØ¨ Ø¹Ù„Ù‰ Ø£Ø·Ø±Ø§ÙÙ‡ ÙŠÙ‚Ø¨Ù„Ù‡Ø§ØŒ Ø«Ù… ØªÙ†Ø­Ù‰ Ø¹Ù†Ù‡ Ù‚Ø§Ø¦Ù…Ø§ØŒ Ø­ØªÙ‰ ÙŠÙ‚Ù ÙÙŠ Ù…Ø±ØªØ¨Ø© Ù…Ø«Ù„Ù‡.\n",
      "\n",
      "Example 3:\n",
      "English: If he signals to him to sit, he sits.\n",
      "Arabic: ÙØ¥Ù† Ø£ÙˆÙ…Ø£ Ø¥Ù„ÙŠÙ‡ Ø¨Ø§Ù„Ù‚Ø¹ÙˆØ¯ØŒ Ù‚Ø¹Ø¯.\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset structure\n",
    "print(\"ğŸ” Dataset Structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Check the first few examples to understand the data format\n",
    "sample_data = dataset['books'][:3]\n",
    "print(\"\\nğŸ“ Sample Data:\")\n",
    "for i, example in enumerate(sample_data['en'][:3]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"English: {example}\")\n",
    "    print(f\"Arabic: {sample_data['ar'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b489542024b0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ Using device: cuda\n",
      "GPU Name: Tesla V100-SXM2-32GB\n",
      "GPU Memory: 34.1 GB\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(7)\n",
    "\n",
    "# Check for GPU availability and set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ğŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "# Display GPU information if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d827019-531b-476b-8c2d-a8336a716c52",
   "metadata": {},
   "source": [
    "## ğŸ”„ Data Preprocessing and Splitting\r\n",
    "\r\n",
    "We'll prepare the data for training by:\r\n",
    "1. **Converting** HuggingFace dataset to pandas DataFrame\r\n",
    "2. **Splitting** into train/test sets (80/20 split)\r\n",
    "3. **Limiting** dataset size for efficient training (5K train, 1K test)\r\n",
    "4. **Creating** HuggingFace Dataset objects for the Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa24ee3b-f57a-4d82-9552-ba9a3df0d186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Dataset Sizes:\n",
      "Training data shape: (5000, 2)\n",
      "Testing data shape: (1000, 2)\n",
      "Total examples: 6000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert HuggingFace dataset to pandas DataFrame for easier manipulation\n",
    "train_data = pd.DataFrame(dataset['books'])\n",
    "\n",
    "# Split data into train and test sets (80/20 split)\n",
    "train_data_full, test_data = train_test_split(\n",
    "    train_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Limit dataset size for efficient training\n",
    "# Using smaller subsets to reduce training time while maintaining quality\n",
    "train_data = train_data_full.head(5000)  # 5K training examples\n",
    "test_data = test_data.head(1000)         # 1K testing examples\n",
    "\n",
    "print(f\"ğŸ“Š Dataset Sizes:\")\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "print(f\"Total examples: {len(train_data) + len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52157bfa-0e90-419e-86d3-938cbe7eecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Sample Training Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ar</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9689</th>\n",
       "      <td>ÙˆÙ„Ù…Ø§ Ø¹Ù‚Ù„ Ù†ÙØ³Ù‡ ÙƒØ§Ù† Ø¹Ø§Ù‚Ù„Ø§ØŒ ÙˆÙ„Ù…Ø§ ÙƒØ§Ù† Ù†ÙØ³Ù‡ Ù…Ø¹Ù‚ÙˆÙ„Ø§ ...</td>\n",
       "      <td>And since he understood himself, he was ration...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7436</th>\n",
       "      <td>Ù…Ù† Ø°Ù„Ùƒ Ù‚ÙˆÙ„Ù‡Ù…: Ø¶Ø±Ø¨Ù‡ Ø¹Ù„Ù‰ Ø£Ù… Ø±Ø£Ø³Ù‡ØŒ ÙˆÙƒØ°Ù„Ùƒ Ø£Ù… Ø§Ù„Ù‡Ø§ÙˆÙŠØ©.</td>\n",
       "      <td>This includes their saying: He hit him on the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>Ù„Ø£Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø£ÙƒØ«Ø± Ù…Ø§ ØªÙ‚Ø¹ ÙˆØªØªÙÙ‚ Ø¨Ø³Ø¤Ø§...</td>\n",
       "      <td>Because these three sections most often occur ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>ÙˆÙ„ÙƒÙ†ÙŠ Ø£Ø³Ø£Ù„Ùƒ Ø£ÙŠÙ‡Ø§ Ø§Ù„Ù…Ù„Ùƒ Ø­Ø§Ø¬Ø© ØªØ³Ø¹ÙÙ†ÙŠ Ø¨Ù‡Ø§ØŒ ÙˆØªØ¹Ø·ÙŠÙ†...</td>\n",
       "      <td>But I ask you, O King, for a need that you may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12394</th>\n",
       "      <td>Ù‚ÙˆÙ„Ù‡: ÙˆØ£ØªØª Ø§Ù„Ø¹Ø¶Ù„ Ø§Ù„Ø¹Ø±ÙŠØ¶Ø© Ø§Ù„Ø­Ù†Ø¬Ø±ÙŠØ© Ø§Ù„ØªÙŠ Ø±Ø¤ÙˆØ³Ù‡Ø§ ...</td>\n",
       "      <td>His saying: â€œAnd the broad laryngeal muscles w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      ar  \\\n",
       "9689   ÙˆÙ„Ù…Ø§ Ø¹Ù‚Ù„ Ù†ÙØ³Ù‡ ÙƒØ§Ù† Ø¹Ø§Ù‚Ù„Ø§ØŒ ÙˆÙ„Ù…Ø§ ÙƒØ§Ù† Ù†ÙØ³Ù‡ Ù…Ø¹Ù‚ÙˆÙ„Ø§ ...   \n",
       "7436   Ù…Ù† Ø°Ù„Ùƒ Ù‚ÙˆÙ„Ù‡Ù…: Ø¶Ø±Ø¨Ù‡ Ø¹Ù„Ù‰ Ø£Ù… Ø±Ø£Ø³Ù‡ØŒ ÙˆÙƒØ°Ù„Ùƒ Ø£Ù… Ø§Ù„Ù‡Ø§ÙˆÙŠØ©.   \n",
       "1082   Ù„Ø£Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø£ÙƒØ«Ø± Ù…Ø§ ØªÙ‚Ø¹ ÙˆØªØªÙÙ‚ Ø¨Ø³Ø¤Ø§...   \n",
       "1671   ÙˆÙ„ÙƒÙ†ÙŠ Ø£Ø³Ø£Ù„Ùƒ Ø£ÙŠÙ‡Ø§ Ø§Ù„Ù…Ù„Ùƒ Ø­Ø§Ø¬Ø© ØªØ³Ø¹ÙÙ†ÙŠ Ø¨Ù‡Ø§ØŒ ÙˆØªØ¹Ø·ÙŠÙ†...   \n",
       "12394  Ù‚ÙˆÙ„Ù‡: ÙˆØ£ØªØª Ø§Ù„Ø¹Ø¶Ù„ Ø§Ù„Ø¹Ø±ÙŠØ¶Ø© Ø§Ù„Ø­Ù†Ø¬Ø±ÙŠØ© Ø§Ù„ØªÙŠ Ø±Ø¤ÙˆØ³Ù‡Ø§ ...   \n",
       "\n",
       "                                                      en  \n",
       "9689   And since he understood himself, he was ration...  \n",
       "7436   This includes their saying: He hit him on the ...  \n",
       "1082   Because these three sections most often occur ...  \n",
       "1671   But I ask you, O King, for a need that you may...  \n",
       "12394  His saying: â€œAnd the broad laryngeal muscles w...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Missing Values Check:\n",
      "English missing: 0\n",
      "Arabic missing: 0\n"
     ]
    }
   ],
   "source": [
    "# Display sample of the training data to verify structure\n",
    "print(\"ğŸ“‹ Sample Training Data:\")\n",
    "display(train_data.head())\n",
    "\n",
    "# Check for any missing values\n",
    "print(f\"\\nğŸ” Missing Values Check:\")\n",
    "print(f\"English missing: {train_data['en'].isnull().sum()}\")\n",
    "print(f\"Arabic missing: {train_data['ar'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c89c94ab-b95f-4caa-ac0d-de9202ab7086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created HuggingFace Datasets:\n",
      "Train dataset: Dataset({\n",
      "    features: ['ar', 'en', '__index_level_0__'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Test dataset: Dataset({\n",
      "    features: ['ar', 'en', '__index_level_0__'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrames to HuggingFace Dataset objects\n",
    "# This format is required for the HuggingFace Trainer API\n",
    "train_dataset = Dataset.from_pandas(train_data, split='train')\n",
    "test_dataset = Dataset.from_pandas(test_data, split='test')\n",
    "\n",
    "print(f\"âœ… Created HuggingFace Datasets:\")\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Test dataset: {test_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99422947-8add-443f-be86-2d08a065eb68",
   "metadata": {},
   "source": [
    "## ğŸ¤– Model and Tokenizer Setup\r\n",
    "\r\n",
    "### Loading Pre-trained mBART Model\r\n",
    "- **Model**: `facebook/mbart-large-50-one-to-many-mmt`\r\n",
    "- **Languages Supported**: 50+ languages including Arabic and English\r\n",
    "- **Architecture**: Encoder-decoder transformer optimized for translation\r\n",
    "- **Pre-training**: Multilingual denoising autoencoder training\r\n",
    "\r\n",
    "### Language Codes Used:\r\n",
    "- **English**: `en_XX`\r\n",
    "- **Arabic**: `ar_AR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20cd2b75-923f-483e-9ae9-a095c9b3eda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading pre-trained mBART model...\n",
      "âœ… Model loaded successfully!\n",
      "Model parameters: 610,879,488\n",
      "Tokenizer vocabulary size: 250054\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "\n",
    "# Load pre-trained mBART model and tokenizer\n",
    "# Using the one-to-many variant which is more efficient for bidirectional translation\n",
    "model_name = 'facebook/mbart-large-50-one-to-many-mmt'\n",
    "\n",
    "print(\"ğŸ”„ Loading pre-trained mBART model...\")\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dcbc53-8ad5-4a75-a26b-fd05702bd03b",
   "metadata": {},
   "source": [
    "## ğŸ”¤ Tokenization and Data Augmentation\r\n",
    "\r\n",
    "### Key Components:\r\n",
    "1. **Language Code Mapping**: Convert `en`/`ar` to mBART tokens (`en_XX`/`ar_AR`)\r\n",
    "2. **Bidirectional Augmentation**: Randomly swap source/target languages\r\n",
    "3. **Proper Tokenization**: Handle source language setting for tokenizer\r\n",
    "4. **Padding & Truncation**: Ensure consistent sequence lengths\r\n",
    "\r\n",
    "### Data Augmentation Strategy:\r\n",
    "- **50% of examples**: English â†’ Arabic\r\n",
    "- **50% of examples**: Arabic â†’ English\r\n",
    "- This creates a balanced bidirectional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64f6341c-7b36-4217-8ff2-17551fbf10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_code(language):\n",
    "    \"\"\"\n",
    "    Convert simple language codes to mBART-specific language tokens.\n",
    "    \n",
    "    Args:\n",
    "        language (str): 'en' or 'ar'\n",
    "    \n",
    "    Returns:\n",
    "        str: mBART language code ('en_XX' or 'ar_AR')\n",
    "    \"\"\"\n",
    "    lang_map = {\n",
    "        'en': 'en_XX',  # English language code for mBART\n",
    "        'ar': 'ar_AR'   # Arabic language code for mBART\n",
    "    }\n",
    "    return lang_map.get(language.lower(), 'en_XX')\n",
    "\n",
    "def get_apt_tokenizer(src_language):\n",
    "    \"\"\"\n",
    "    Configure tokenizer with appropriate source language setting.\n",
    "    \n",
    "    Args:\n",
    "        src_language (str): Source language ('en' or 'ar')\n",
    "    \n",
    "    Returns:\n",
    "        tokenizer: Configured tokenizer object\n",
    "    \"\"\"\n",
    "    lang_code = get_lang_code(src_language)\n",
    "    tokenizer.src_lang = lang_code\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_function(rows):\n",
    "    \"\"\"\n",
    "    Tokenize input data with bidirectional data augmentation.\n",
    "    \n",
    "    This function:\n",
    "    1. Randomly selects translation direction (enâ†’ar or arâ†’en)\n",
    "    2. Tokenizes source text with proper language codes\n",
    "    3. Tokenizes target text as labels\n",
    "    4. Applies padding and truncation\n",
    "    \n",
    "    Args:\n",
    "        rows (dict): Batch of data containing 'en' and 'ar' keys\n",
    "    \n",
    "    Returns:\n",
    "        dict: Tokenized data with input_ids, attention_mask, and labels\n",
    "    \"\"\"\n",
    "    # Start with English to Arabic as default\n",
    "    src_language = 'en'\n",
    "    tgt_language = 'ar'\n",
    "\n",
    "    # ğŸ² Data Augmentation: Randomly swap source and target languages\n",
    "    # This creates a balanced bidirectional training dataset\n",
    "    if random.randint(0, 1) == 1:\n",
    "        src_language, tgt_language = tgt_language, src_language\n",
    "\n",
    "    # Get mBART language codes\n",
    "    src_lang_code = get_lang_code(src_language)\n",
    "    tgt_lang_code = get_lang_code(tgt_language)\n",
    "    \n",
    "    # Set source language for tokenizer\n",
    "    tokenizer.src_lang = src_lang_code\n",
    "    \n",
    "    # Prepare input prompts (currently no special prompting)\n",
    "    start_prompt = ''\n",
    "    end_prompt = ''\n",
    "    prompt = [start_prompt + row + end_prompt for row in rows[src_language]]\n",
    "    \n",
    "    # ğŸ”¤ Tokenize input text (source language)\n",
    "    encoding = tokenizer(\n",
    "        prompt, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=256  # Maximum sequence length\n",
    "    )\n",
    "    \n",
    "    # ğŸ”¤ Tokenize target text (labels)\n",
    "    tokenizer.src_lang = tgt_lang_code  # Set target language for proper tokenization\n",
    "    labels = tokenizer(\n",
    "        rows[tgt_language], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    # âœ… Convert tensors to lists for HuggingFace datasets compatibility\n",
    "    # The trainer will convert them back to tensors during training\n",
    "    return {\n",
    "        'input_ids': encoding.input_ids.tolist(),\n",
    "        'attention_mask': encoding.attention_mask.tolist(),\n",
    "        'labels': labels.input_ids.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e3745-ff76-4b17-893e-b6deefd9f40c",
   "metadata": {},
   "source": [
    "## âš™ï¸ Applying Tokenization to Datasets\r\n",
    "\r\n",
    "Now we'll apply our tokenization function to both training and testing datasets. This process:\r\n",
    "- Converts text to numerical tokens\r\n",
    "- Applies bidirectional data augmentation\r\n",
    "- Creates properly formatted inputs for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e30a2a4f-90a1-49ff-8707-de89c0106e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c0cb941b31452c85f8a1f60b590edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing training data:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b9789ecec24d8599baae3e2d8dd521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing test data:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenization completed!\n",
      "Tokenized train dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Tokenized test dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Train dataset columns: ['input_ids', 'attention_mask', 'labels']\n",
      "Test dataset columns: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization to training dataset\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    desc=\"Tokenizing training data\"\n",
    ")\n",
    "\n",
    "# Apply tokenization to test dataset\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    desc=\"Tokenizing test data\"\n",
    ")\n",
    "\n",
    "# ğŸ”§ IMPORTANT: Remove columns that contain raw text to avoid training errors\n",
    "# Keep only the columns needed for training: input_ids, attention_mask, labels\n",
    "columns_to_remove = ['en', 'ar', 'src_language', 'prompt', '__index_level_0__']\n",
    "\n",
    "# Remove unnecessary columns from training dataset\n",
    "for col in columns_to_remove:\n",
    "    if col in tokenized_train_dataset.column_names:\n",
    "        tokenized_train_dataset = tokenized_train_dataset.remove_columns([col])\n",
    "\n",
    "# Remove unnecessary columns from test dataset  \n",
    "for col in columns_to_remove:\n",
    "    if col in tokenized_test_dataset.column_names:\n",
    "        tokenized_test_dataset = tokenized_test_dataset.remove_columns([col])\n",
    "\n",
    "print(\"âœ… Tokenization completed!\")\n",
    "print(f\"Tokenized train dataset: {tokenized_train_dataset}\")\n",
    "print(f\"Tokenized test dataset: {tokenized_test_dataset}\")\n",
    "print(f\"Train dataset columns: {tokenized_train_dataset.column_names}\")\n",
    "print(f\"Test dataset columns: {tokenized_test_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1359b-0876-4a70-b021-52fdeabc8d88",
   "metadata": {},
   "source": [
    "## ğŸ” Verifying Tokenization\n",
    "\n",
    "Let's examine a tokenized example to ensure our tokenization process works correctly:\n",
    "- Check source language assignment\n",
    "- Verify input and label tokenization\n",
    "- Confirm bidirectional augmentation is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e306a21-ad1c-43d5-81db-de3394d97805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Tokenization Verification:\n",
      "Original English text: He apologized to him and said: The monkeys do not own anything, but sit down until I come to you.\n",
      "Original Arabic text: ÙˆØ§Ø¹ØªØ°Ø± Ø¥Ù„ÙŠÙ‡ØŒ ÙˆÙ‚Ø§Ù„: Ø¥Ù† Ø§Ù„Ù‚Ø±ÙˆØ¯ Ù„Ø§ ÙŠÙ…Ù„ÙƒÙˆÙ† Ø´ÙŠØ¦Ø§ØŒ ÙˆÙ„ÙƒÙ† Ø§Ù‚Ø¹Ø¯ Ø­ØªÙ‰ Ø¢ØªÙŠÙƒ.\n",
      "\n",
      "ğŸ“¥ Tokenized Input (first 10 tokens):\n",
      "Input IDs length: 256\n",
      "Input IDs (first 10): [250004, 1529, 1747, 14071, 29367, 47, 4049, 136, 2804, 12]\n",
      "\n",
      "ğŸ“¤ Tokenized Labels (first 10 tokens):\n",
      "Labels length: 256\n",
      "Labels (first 10): [250001, 3138, 11429, 31687, 42364, 50, 18789, 12, 5202, 51218]\n",
      "\n",
      "ğŸ”¢ Attention mask length: 256\n",
      "Attention mask (first 10): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "âœ… Dataset columns after cleaning: ['input_ids', 'attention_mask', 'labels']\n",
      "\n",
      "ğŸ” Data Types:\n",
      "Input IDs type: <class 'list'>\n",
      "Labels type: <class 'list'>\n",
      "Attention mask type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# We need to get an example differently since we removed the original text columns\n",
    "# Let's get the original data for verification\n",
    "original_test_example = test_dataset[3]  # Get from original dataset\n",
    "tokenized_example = tokenized_test_dataset[3]  # Get from tokenized dataset\n",
    "\n",
    "print(\"ğŸ” Tokenization Verification:\")\n",
    "print(f\"Original English text: {original_test_example['en']}\")\n",
    "print(f\"Original Arabic text: {original_test_example['ar']}\")\n",
    "print()\n",
    "print(\"ğŸ“¥ Tokenized Input (first 10 tokens):\")\n",
    "print(f\"Input IDs length: {len(tokenized_example['input_ids'])}\")\n",
    "print(f\"Input IDs (first 10): {tokenized_example['input_ids'][:10]}\")\n",
    "print()\n",
    "print(\"ğŸ“¤ Tokenized Labels (first 10 tokens):\")\n",
    "print(f\"Labels length: {len(tokenized_example['labels'])}\")\n",
    "print(f\"Labels (first 10): {tokenized_example['labels'][:10]}\")\n",
    "print()\n",
    "print(\"ğŸ”¢ Attention mask length:\", len(tokenized_example['attention_mask']))\n",
    "print(f\"Attention mask (first 10): {tokenized_example['attention_mask'][:10]}\")\n",
    "\n",
    "# Verify we only have the necessary columns\n",
    "print(f\"\\nâœ… Dataset columns after cleaning: {tokenized_test_dataset.column_names}\")\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nğŸ” Data Types:\")\n",
    "print(f\"Input IDs type: {type(tokenized_example['input_ids'])}\")\n",
    "print(f\"Labels type: {type(tokenized_example['labels'])}\")\n",
    "print(f\"Attention mask type: {type(tokenized_example['attention_mask'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55b64558-370c-47ba-b122-fc4a65d70249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”€ Datasets shuffled successfully!\n",
      "Final training dataset shape: (5000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle datasets to ensure random distribution during training\n",
    "# This helps prevent overfitting to data ordering patterns\n",
    "tokenized_train_dataset = tokenized_train_dataset.shuffle(seed=13)\n",
    "tokenized_test_dataset = tokenized_test_dataset.shuffle(seed=13)\n",
    "\n",
    "print(\"ğŸ”€ Datasets shuffled successfully!\")\n",
    "print(f\"Final training dataset shape: {tokenized_train_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0891017-5569-4d54-903f-7bc2b9ece8f3",
   "metadata": {},
   "source": [
    "## ğŸ§ª Pre-training Model Evaluation\n",
    "\n",
    "Before fine-tuning, let's test the pre-trained model's translation capabilities:\n",
    "- **English â†’ Arabic**: Test with sample English text\n",
    "- **Arabic â†’ English**: Test with sample Arabic text\n",
    "- This establishes baseline performance for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57bd9cb8-e953-436b-86f0-8e9445b277e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Testing English â†’ Arabic translation (pre-trained):\n",
      "Input: He apologized to him and said: The monkeys do not own anything, but sit down until I come to you.\n",
      "Translation: ÙØ£Ø¹ØªØ°Ø± Ø¥Ù„ÙŠÙ‡ØŒ ÙˆÙ‚Ø§Ù„: Ø¥Ù† Ø§Ù„Ù‚Ø±Ø¯Ø© Ù„Ø§ ØªÙ…Ù„Ùƒ Ø´ÙŠØ¦Ø§ØŒ Ø¥Ù†Ù…Ø§ ØªØ¬Ù„Ø³ Ø¥Ù„Ù‰ Ø£Ù† Ø¢ØªÙŠ Ø¥Ù„ÙŠÙƒ.\n"
     ]
    }
   ],
   "source": [
    "# Test English to Arabic translation with pre-trained model\n",
    "sample_english = \"He apologized to him and said: The monkeys do not own anything, but sit down until I come to you.\"\n",
    "\n",
    "print(\"ğŸ”„ Testing English â†’ Arabic translation (pre-trained):\")\n",
    "print(f\"Input: {sample_english}\")\n",
    "\n",
    "# Set tokenizer source language to English\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "\n",
    "# Generate Arabic translation\n",
    "with torch.no_grad():\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(sample_english, return_tensors='pt')\n",
    "    \n",
    "    # ğŸ”§ IMPORTANT: Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate translation with forced Arabic output\n",
    "    result = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"ar_AR\"],  # Force Arabic output\n",
    "        max_length=512  # Maximum output length\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    translation = tokenizer.batch_decode(result, skip_special_tokens=True)[0]\n",
    "\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "272a1ea0-2c97-4c26-87f0-33c8bbe6d258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Testing Arabic â†’ English translation (pre-trained):\n",
      "Input: ÙˆØ§Ø¹ØªØ°Ø± Ø¥Ù„ÙŠÙ‡ØŒ ÙˆÙ‚Ø§Ù„: Ø¥Ù† Ø§Ù„Ù‚Ø±ÙˆØ¯ Ù„Ø§ ÙŠÙ…Ù„ÙƒÙˆÙ† Ø´ÙŠØ¦Ø§ØŒ ÙˆÙ„ÙƒÙ† Ø§Ù‚Ø¹Ø¯ Ø­ØªÙ‰ Ø¢ØªÙŠÙƒ.\n",
      "Translation: He agreed to him and said: The crown has nothing to have, but he is deceived until he has got it.\n"
     ]
    }
   ],
   "source": [
    "# Test Arabic to English translation with pre-trained model\n",
    "sample_arabic = \"ÙˆØ§Ø¹ØªØ°Ø± Ø¥Ù„ÙŠÙ‡ØŒ ÙˆÙ‚Ø§Ù„: Ø¥Ù† Ø§Ù„Ù‚Ø±ÙˆØ¯ Ù„Ø§ ÙŠÙ…Ù„ÙƒÙˆÙ† Ø´ÙŠØ¦Ø§ØŒ ÙˆÙ„ÙƒÙ† Ø§Ù‚Ø¹Ø¯ Ø­ØªÙ‰ Ø¢ØªÙŠÙƒ.\"\n",
    "\n",
    "print(\"ğŸ”„ Testing Arabic â†’ English translation (pre-trained):\")\n",
    "print(f\"Input: {sample_arabic}\")\n",
    "\n",
    "# Set tokenizer source language to Arabic\n",
    "tokenizer.src_lang = \"ar_AR\"\n",
    "\n",
    "# Generate English translation\n",
    "with torch.no_grad():\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(sample_arabic, return_tensors='pt')\n",
    "    \n",
    "    # ğŸ”§ IMPORTANT: Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate translation with forced English output\n",
    "    result = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"],  # Force English output\n",
    "        max_length=512  # Maximum output length\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    translation = tokenizer.batch_decode(result, skip_special_tokens=True)[0]\n",
    "\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18023a9-90ca-44d1-bd8c-f7c48d9ffb66",
   "metadata": {},
   "source": [
    "## ğŸ‹ï¸ Training Configuration and Setup\n",
    "\n",
    "### Training Parameters:\n",
    "- **Epochs**: 2 (sufficient for fine-tuning pre-trained model)\n",
    "- **Learning Rate**: 5e-5 (conservative for stable fine-tuning)\n",
    "- **Batch Size**: Auto-detected based on GPU memory\n",
    "- **Evaluation**: After each epoch\n",
    "- **Warmup Steps**: 100 (gradual learning rate increase)\n",
    "\n",
    "### Training Strategy:\n",
    "- **Fine-tuning**: Adjust pre-trained weights rather than training from scratch\n",
    "- **Mixed Precision**: Automatic optimization for faster training\n",
    "- **Gradient Accumulation**: Handled automatically by Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44ac5b59-ae92-4f97-8ef8-f5a502b83e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training configuration completed!\n",
      "Output directory: ./checkpoints/mbart-translator-full-run\n",
      "Training examples: 5000\n",
      "Validation examples: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1421368/1792062125.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store training metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Define output directory for model checkpoints\n",
    "output_dir = f'./checkpoints/mbart-translator-full-run'\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,                    # Directory to save model checkpoints\n",
    "    auto_find_batch_size=True,               # Automatically find optimal batch size\n",
    "    learning_rate=5e-5,                      # Conservative learning rate for fine-tuning\n",
    "    num_train_epochs=2,                      # Number of training epochs\n",
    "    eval_strategy=\"epoch\",                   # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",                   # Save checkpoint after each epoch\n",
    "    logging_strategy=\"epoch\",                # Log metrics after each epoch\n",
    "    warmup_steps=100,                        # Gradual learning rate warmup\n",
    "    report_to=\"none\",                        # Disable external logging (W&B, etc.)\n",
    "    load_best_model_at_end=True,            # Load best checkpoint at end\n",
    "    metric_for_best_model=\"eval_loss\",       # Use validation loss to select best model\n",
    "    greater_is_better=False,                 # Lower loss is better\n",
    "    dataloader_pin_memory=True,              # Speed up data loading\n",
    "    remove_unused_columns=False,             # Keep all columns for custom processing\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                            # Model to train\n",
    "    args=training_args,                     # Training configuration\n",
    "    train_dataset=tokenized_train_dataset,  # Training data\n",
    "    eval_dataset=tokenized_test_dataset,    # Validation data\n",
    "    tokenizer=tokenizer                     # Tokenizer for proper saving\n",
    ")\n",
    "\n",
    "print(\"âœ… Training configuration completed!\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Training examples: {len(tokenized_train_dataset)}\")\n",
    "print(f\"Validation examples: {len(tokenized_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd3100-f529-4b42-b579-9f9d513da103",
   "metadata": {},
   "source": [
    "## ğŸš€ Model Training\r\n",
    "\r\n",
    "Starting the fine-tuning process. This will:\r\n",
    "1. **Fine-tune** the pre-trained mBART model on our Arabic-English dataset\r\n",
    "2. **Evaluate** performance after each epoch\r\n",
    "3. **Save** checkpoints for the best performing model\r\n",
    "4. **Track** training and validation losses\r\n",
    "\r\n",
    "**Expected Duration**: ~10-20 minutes depending on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bef61f6a-2660-4e2e-9b4c-e2b0befff87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 19:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.375500</td>\n",
       "      <td>0.357682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.288600</td>\n",
       "      <td>0.343950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ibex/user/habiam0b/miniconda3/envs/GPU/lib/python3.9/site-packages/transformers/modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.832045361328125, metrics={'train_runtime': 1147.1685, 'train_samples_per_second': 8.717, 'train_steps_per_second': 1.09, 'total_flos': 5417824419840000.0, 'train_loss': 0.832045361328125, 'epoch': 2.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training process\n",
    "# The trainer will automatically handle:\n",
    "# - Forward and backward passes\n",
    "# - Loss computation\n",
    "# - Optimizer steps\n",
    "# - Learning rate scheduling\n",
    "# - Evaluation loops\n",
    "# - Checkpoint saving\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2b1c3-55ca-436d-b997-6e97d792fedd",
   "metadata": {},
   "source": [
    "## ğŸ“Š Training Metrics Visualization\r\n",
    "\r\n",
    "Let's extract and visualize the training history to understand:\r\n",
    "- **Training Loss**: How well the model fits the training data\r\n",
    "- **Validation Loss**: How well the model generalizes to unseen data\r\n",
    "- **Training Progress**: Convergence patterns over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17651ff3-01f0-4e4d-bfd4-a57c9af7cce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Extracting training metrics...\n",
      "ğŸ“ˆ Training Metrics:\n",
      "Training losses: [1.3755, 0.2886]\n",
      "Validation losses: [0.35768234729766846, 0.3439500033855438]\n",
      "Epochs: [1.0, 2.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Training Analysis:\n",
      "Training loss improvement: 1.0869\n",
      "Validation loss improvement: 0.0137\n"
     ]
    }
   ],
   "source": [
    "# Extract training metrics from trainer state\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs = []\n",
    "\n",
    "print(\"ğŸ“Š Extracting training metrics...\")\n",
    "\n",
    "# Parse through training log history\n",
    "for log in trainer.state.log_history:\n",
    "    if \"epoch\" in log:\n",
    "        if \"loss\" in log and \"eval_loss\" not in log:\n",
    "            # Training loss\n",
    "            train_losses.append(log[\"loss\"])\n",
    "            epochs.append(log[\"epoch\"])\n",
    "        elif \"eval_loss\" in log:\n",
    "            # Validation loss\n",
    "            val_losses.append(log[\"eval_loss\"])\n",
    "\n",
    "# Display extracted metrics\n",
    "print(\"ğŸ“ˆ Training Metrics:\")\n",
    "print(f\"Training losses: {train_losses}\")\n",
    "print(f\"Validation losses: {val_losses}\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "\n",
    "# Create loss visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, 'b-o', label='Training Loss', linewidth=2, markersize=6)\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-s', label='Validation Loss', linewidth=2, markersize=6)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training and Validation Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add text annotations for final values\n",
    "if train_losses and val_losses:\n",
    "    plt.annotate(f'Final Train: {train_losses[-1]:.4f}', \n",
    "                xy=(epochs[-1], train_losses[-1]), xytext=(10, 10),\n",
    "                textcoords='offset points', fontsize=10,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='blue', alpha=0.3))\n",
    "    plt.annotate(f'Final Val: {val_losses[-1]:.4f}', \n",
    "                xy=(len(val_losses), val_losses[-1]), xytext=(10, -20),\n",
    "                textcoords='offset points', fontsize=10,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='red', alpha=0.3))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Analyze training results\n",
    "print(\"\\nğŸ¯ Training Analysis:\")\n",
    "if len(train_losses) > 1:\n",
    "    improvement = train_losses[0] - train_losses[-1]\n",
    "    print(f\"Training loss improvement: {improvement:.4f}\")\n",
    "if len(val_losses) > 1:\n",
    "    val_improvement = val_losses[0] - val_losses[-1]\n",
    "    print(f\"Validation loss improvement: {val_improvement:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab29bae-42ef-4654-bb95-8385e784be14",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Saving the Fine-tuned Model\n",
    "\n",
    "Saving both the model and tokenizer for future use:\n",
    "- **Model weights**: Fine-tuned parameters\n",
    "- **Tokenizer configuration**: Language mappings and vocabulary\n",
    "- **Generation config**: Default inference parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee5879d8-69e8-428b-8472-6f4ecf57911c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving fine-tuned model to: mbart_one_to_many_2ep_full_finetuned_bidir\n",
      "âœ… Model and tokenizer saved successfully!\n",
      "ğŸ“ Saved files in: mbart_one_to_many_2ep_full_finetuned_bidir/\n",
      "ğŸ“‹ Saved files:\n",
      "  - config.json\n",
      "  - generation_config.json\n",
      "  - model.safetensors\n",
      "  - sentencepiece.bpe.model\n",
      "  - special_tokens_map.json\n",
      "  - tokenizer_config.json\n",
      "  - training_args.bin\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model_save_path = 'mbart_one_to_many_2ep_full_finetuned_bidir'\n",
    "\n",
    "print(f\"ğŸ’¾ Saving fine-tuned model to: {model_save_path}\")\n",
    "\n",
    "# Save both model and tokenizer\n",
    "# This automatically saves:\n",
    "# - Model weights (pytorch_model.bin)\n",
    "# - Model configuration (config.json)\n",
    "# - Tokenizer files (tokenizer.json, special_tokens_map.json, etc.)\n",
    "trainer.save_model(model_save_path)\n",
    "\n",
    "print(\"âœ… Model and tokenizer saved successfully!\")\n",
    "print(f\"ğŸ“ Saved files in: {model_save_path}/\")\n",
    "\n",
    "# List saved files\n",
    "import os\n",
    "if os.path.exists(model_save_path):\n",
    "    files = os.listdir(model_save_path)\n",
    "    print(\"ğŸ“‹ Saved files:\")\n",
    "    for file in sorted(files):\n",
    "        print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d1897-e6b7-44f4-a858-60c3d3c9224b",
   "metadata": {},
   "source": [
    "## ğŸ”„ Loading the Fine-tuned Model\n",
    "\n",
    "Loading our saved model for inference and evaluation:\n",
    "- **Model**: Fine-tuned mBART with improved Arabic-English translation\n",
    "- **Tokenizer**: Configured with proper language mappings\n",
    "- **Device**: Moving to GPU for faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b252e4c-3c59-46e1-9e8e-ec7ccb2463ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading fine-tuned model...\n",
      "âœ… Fine-tuned model loaded successfully!\n",
      "ğŸ–¥ï¸ Model device: cuda:0\n",
      "ğŸ“Š Model parameters: 610,879,488\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "\n",
    "print(\"ğŸ”„ Loading fine-tuned model...\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer from saved directory\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_save_path)\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_save_path)\n",
    "\n",
    "# Move model to appropriate device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "print(\"âœ… Fine-tuned model loaded successfully!\")\n",
    "print(f\"ğŸ–¥ï¸ Model device: {next(model.parameters()).device}\")\n",
    "print(f\"ğŸ“Š Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc3939-3000-439a-a414-0bee4579aa00",
   "metadata": {},
   "source": [
    "## ğŸ§ª Post-Training Model Evaluation\n",
    "\n",
    "Testing our fine-tuned model with the same examples used for pre-training evaluation:\n",
    "- **Compare** performance before and after fine-tuning\n",
    "- **Assess** translation quality improvements\n",
    "- **Verify** bidirectional translation capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ed9aad4-ccd4-4a57-98f0-fb5ac43ac2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Testing English â†’ Arabic translation (fine-tuned):\n",
      "Input: The greater the burden, the more you must be charged with it by God. The greater the blessing He has bestowed upon you, the more you must be thankful\n",
      "Translation: ÙˆÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ø§Ù„Ø«Ù‚Ù„ Ø£Ø¹Ø¸Ù…ØŒ ÙƒØ§Ù† Ø¹Ù„ÙŠÙƒ Ø£Ù† ØªÙƒÙ„ÙÙ‡ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ù„Ù‡ØŒ ÙˆÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ø¹Ù…Ø© Ø§Ù„ØªÙŠ Ø£Ù†Ø¬Ø² Ø¹Ù„ÙŠÙƒ Ø£Ø¹Ø¸Ù…ØŒ ÙƒØ§Ù† Ø¹Ù„ÙŠÙƒ Ø£Ù† ØªØ´ÙƒØ± Ø£ÙƒØ«Ø±.\n"
     ]
    }
   ],
   "source": [
    "# Test fine-tuned model: English â†’ Arabic translation\n",
    "sample_english = \"The greater the burden, the more you must be charged with it by God. The greater the blessing He has bestowed upon you, the more you must be thankful\"\n",
    "\n",
    "print(\"ğŸ”„ Testing English â†’ Arabic translation (fine-tuned):\")\n",
    "print(f\"Input: {sample_english}\")\n",
    "\n",
    "# Set tokenizer source language to English\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "\n",
    "# Generate Arabic translation using fine-tuned model\n",
    "with torch.no_grad():\n",
    "    # Tokenize input text\n",
    "    encoding = tokenizer(sample_english, return_tensors='pt')\n",
    "    \n",
    "    # ğŸ”§ IMPORTANT: Move inputs to the same device as the model\n",
    "    input_ids = encoding.input_ids.to(device)\n",
    "    attention_mask = encoding.attention_mask.to(device)\n",
    "    \n",
    "    # Generate translation with forced Arabic output\n",
    "    result = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"ar_AR\"],  # Force Arabic output\n",
    "        max_length=512,\n",
    "        num_beams=4,  # Beam search for better quality\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    translation = tokenizer.batch_decode(result, skip_special_tokens=True)[0]\n",
    "\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b82e3129-660d-4054-a657-30d2c8fcdc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Testing Arabic â†’ English translation (fine-tuned):\n",
      "Input: ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯ Ø§Ù„Ø¹Ø¨Ø¡ØŒ ÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ø¹Ù„ÙŠÙƒ Ø£Ù† ØªØªØ­Ù…Ù„Ù‡ Ù…Ù† Ø§Ù„Ù„Ù‡. ÙƒÙ„Ù…Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨Ø±ÙƒØ© Ø§Ù„ØªÙŠ Ù…Ù†Ø­Ù‡Ø§ Ù„Ùƒ Ø£ÙƒØ¨Ø±ØŒ ÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ø¹Ù„ÙŠÙƒ Ø£Ù† ØªÙƒÙˆÙ† Ø£ÙƒØ«Ø± Ø§Ù…ØªÙ†Ø§Ù†Ù‹Ø§\n",
      "Translation: However you increase the intellect, however you support him from God. However the fruits that give it to you are greater, however you become more solid, however you become more solid\n"
     ]
    }
   ],
   "source": [
    "# Test fine-tuned model: Arabic â†’ English translation\n",
    "sample_arabic = \"ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯ Ø§Ù„Ø¹Ø¨Ø¡ØŒ ÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ø¹Ù„ÙŠÙƒ Ø£Ù† ØªØªØ­Ù…Ù„Ù‡ Ù…Ù† Ø§Ù„Ù„Ù‡. ÙƒÙ„Ù…Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨Ø±ÙƒØ© Ø§Ù„ØªÙŠ Ù…Ù†Ø­Ù‡Ø§ Ù„Ùƒ Ø£ÙƒØ¨Ø±ØŒ ÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ø¹Ù„ÙŠÙƒ Ø£Ù† ØªÙƒÙˆÙ† Ø£ÙƒØ«Ø± Ø§Ù…ØªÙ†Ø§Ù†Ù‹Ø§\"\n",
    "\n",
    "print(\"ğŸ”„ Testing Arabic â†’ English translation (fine-tuned):\")\n",
    "print(f\"Input: {sample_arabic}\")\n",
    "\n",
    "# Set tokenizer source language to Arabic\n",
    "tokenizer.src_lang = \"ar_AR\"\n",
    "\n",
    "# Generate English translation using fine-tuned model\n",
    "with torch.no_grad():\n",
    "    # Tokenize input text\n",
    "    encoding = tokenizer(sample_arabic, return_tensors='pt')\n",
    "    \n",
    "    # ğŸ”§ IMPORTANT: Move inputs to the same device as the model\n",
    "    input_ids = encoding.input_ids.to(device)\n",
    "    attention_mask = encoding.attention_mask.to(device)\n",
    "    \n",
    "    # Generate translation with forced English output\n",
    "    result = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"],  # Force English output\n",
    "        max_length=512,\n",
    "        num_beams=4,  # Beam search for better quality\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    translation = tokenizer.batch_decode(result, skip_special_tokens=True)[0]\n",
    "\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c99ab8a-f9cf-44ef-82c5-7abdce3541e3",
   "metadata": {},
   "source": [
    "## ğŸ“ Quantitative Evaluation with BLEU Score\n",
    "\n",
    "**BLEU (Bilingual Evaluation Understudy)** is a standard metric for evaluating translation quality:\n",
    "- **Range**: 0-100 (higher is better)\n",
    "- **Measurement**: N-gram overlap between predicted and reference translations\n",
    "- **Comparison**: We'll compare pre-trained vs. fine-tuned model performance\n",
    "\n",
    "### Evaluation Setup:\n",
    "- **Test Set**: 500 Englishâ†’Arabic pairs from our test data\n",
    "- **Metrics**: BLEU-1, BLEU-2, BLEU-3, BLEU-4 scores\n",
    "- **Models**: Original mBART vs. Fine-tuned mBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b92eb43-1c93-4562-aa2b-ff94aadc81bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf7704cdd76469c803d08c35a979458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4990273f16b465c821ffbcd5a599d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Setting up BLEU evaluation...\n",
      "ğŸ”„ Loading original mBART model for comparison...\n",
      "âœ… Original model and tokenizer loaded for comparison!\n",
      "ğŸ–¥ï¸ Untrained model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load BLEU evaluation metric\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "print(\"ğŸ“ Setting up BLEU evaluation...\")\n",
    "\n",
    "# Load the original (untrained) model for comparison\n",
    "print(\"ğŸ”„ Loading original mBART model for comparison...\")\n",
    "untrained_model = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-one-to-many-mmt')\n",
    "untrained_model.to(device)\n",
    "\n",
    "# Load original tokenizer\n",
    "untrained_tokenizer = MBart50Tokenizer.from_pretrained('facebook/mbart-large-50-one-to-many-mmt')\n",
    "\n",
    "print(\"âœ… Original model and tokenizer loaded for comparison!\")\n",
    "print(f\"ğŸ–¥ï¸ Untrained model device: {next(untrained_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6a5324-5837-4462-86aa-e54ccd6ac0e5",
   "metadata": {},
   "source": [
    "## ğŸ¯ Evaluation Data Preparation\r\n",
    "\r\n",
    "For fair comparison, we'll split our test data into two groups:\r\n",
    "- **English â†’ Arabic**: 500 translation pairs\r\n",
    "- **Arabic â†’ English**: 500 translation pairs\r\n",
    "\r\n",
    "This ensures balanced evaluation of both translation directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8086ee3-18bd-4a46-b5dc-0f014317f0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Evaluation Data Split:\n",
      "English â†’ Arabic pairs: 500\n",
      "Arabic â†’ English pairs: 500\n",
      "Total evaluation pairs: 1000\n",
      "\n",
      "ğŸ“ Sample English â†’ Arabic:\n",
      "EN: What appears from these words is that the nerve exits in the first vertebra from above, and this was invalidated in his words in the first vertebra, and it is apparently invalidated by what he said in his words in the first vertebra.\n",
      "AR: Ø§Ù„Ø°ÙŠ ÙŠØ¸Ù‡Ø± Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ù… Ø£Ù† Ù…Ø®Ø±Ø¬ Ø§Ù„Ø¹ØµØ¨ ÙÙŠ Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ù† ÙÙˆÙ‚ ÙˆÙ‡Ø°Ø§ Ù‚Ø¯ Ø£Ø¨Ø·Ù„Ù‡ ÙÙŠ ÙƒÙ„Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙˆÙ‡Ùˆ Ø¸Ø§Ù‡Ø± Ø§Ù„Ø¨Ø·Ù„Ø§Ù† Ø¨Ù…Ø§ Ù‚Ø§Ù„Ù‡ ÙÙŠ ÙƒÙ„Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰.\n",
      "\n",
      "ğŸ“ Sample Arabic â†’ English:\n",
      "AR: [ÙƒÙŠÙÙŠØ© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ùƒ] ÙˆØ¥Ù† ÙƒØ§Ù† Ø¯Ø®ÙˆÙ„Ù‡ Ø¹Ù„ÙŠÙ‡ Ù…Ù† Ø§Ù„Ø¨Ø§Ø¨ Ø§Ù„Ø£ÙˆÙ„ Ø§Ù„Ø°ÙŠ ÙŠÙ‚Ø§Ø¨Ù„ ÙˆØ¬Ù‡ Ø§Ù„Ù…Ù„Ùƒ ÙˆÙŠØ­Ø§Ø°ÙŠÙ‡ - ÙˆÙƒØ§Ù† Ù„Ù‡ Ø·Ø±ÙŠÙ‚ Ø¹Ù† ÙŠÙ…ÙŠÙ†Ù‡ Ø£Ùˆ Ø´Ù…Ø§Ù„Ù‡ - Ø¹Ø¯Ù„ Ù†Ø­Ùˆ Ø§Ù„Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø°ÙŠ Ù„Ø§ ÙŠÙ‚Ø§Ø¨Ù„Ù‡ ÙÙŠÙ‡ Ø¨ÙˆØ¬Ù‡Ù‡ØŒ Ø«Ù… Ø§Ù†Ø­Ø±Ù Ù†Ø­Ùˆ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ù„ÙƒØŒ ÙØ³Ù„Ù… Ù‚Ø§Ø¦Ù…Ø§ Ù…Ù„Ø§Ø­Ø¸Ø§ Ù„Ù„Ù…Ù„Ùƒ.\n",
      "EN: [How to enter upon the king] If he enters upon him from the first door that faces the kingâ€™s face and is parallel to him - and he has a path to his right or left - he turns towards the path that does not face him, then turns towards the kingâ€™s seat, and greets him while standing and observing the king.\n"
     ]
    }
   ],
   "source": [
    "# Split test data for bidirectional evaluation\n",
    "# This ensures we test both translation directions equally\n",
    "en_to_ar_data = test_data.sample(frac=0.5, random_state=123)\n",
    "ar_to_en_data = test_data.loc[test_data.index.difference(en_to_ar_data.index)]\n",
    "\n",
    "print(f\"ğŸ“Š Evaluation Data Split:\")\n",
    "print(f\"English â†’ Arabic pairs: {len(en_to_ar_data)}\")\n",
    "print(f\"Arabic â†’ English pairs: {len(ar_to_en_data)}\")\n",
    "print(f\"Total evaluation pairs: {len(en_to_ar_data) + len(ar_to_en_data)}\")\n",
    "\n",
    "# Display sample from each direction\n",
    "print(f\"\\nğŸ“ Sample English â†’ Arabic:\")\n",
    "sample_en = en_to_ar_data.iloc[0]\n",
    "print(f\"EN: {sample_en['en']}\")\n",
    "print(f\"AR: {sample_en['ar']}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Sample Arabic â†’ English:\")\n",
    "sample_ar = ar_to_en_data.iloc[0]\n",
    "print(f\"AR: {sample_ar['ar']}\")\n",
    "print(f\"EN: {sample_ar['en']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e021e7b-bcfb-41b0-a5bb-c6a898582279",
   "metadata": {},
   "source": [
    "## ğŸ”„ English â†’ Arabic Translation Evaluation\n",
    "\n",
    "We'll process the Englishâ†’Arabic test set in batches to:\n",
    "1. **Generate translations** from both untrained and fine-tuned models\n",
    "2. **Compare outputs** with reference Arabic translations\n",
    "3. **Compute BLEU scores** for quantitative assessment\n",
    "\n",
    "**Batch Processing**: Using batches of 16 for efficient GPU utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61936017-0182-46ad-8552-20d28e23c8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing 500 English â†’ Arabic translations...\n",
      "Using 32 batches of size 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating translations:   0%|                                                                   | 0/32 [00:00<?, ?it/s]`generation_config` default values have been modified to match model-specific defaults: {'max_length': 200, 'forced_eos_token_id': 2, 'pad_token_id': 1, 'bos_token_id': 0, 'eos_token_id': 2, 'decoder_start_token_id': 2}. If this is not desired, please set these values explicitly.\n",
      "Generating translations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [05:38<00:00, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 500 untrained translations\n",
      "âœ… Generated 500 fine-tuned translations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set batch size for efficient processing\n",
    "batch_size = 16\n",
    "num_batches = int(math.ceil(len(en_to_ar_data) / batch_size))\n",
    "\n",
    "# Store all model outputs for BLEU computation\n",
    "all_untrained_model_outputs = []\n",
    "all_trained_model_outputs = []\n",
    "\n",
    "print(f\"ğŸ”„ Processing {len(en_to_ar_data)} English â†’ Arabic translations...\")\n",
    "print(f\"Using {num_batches} batches of size {batch_size}\")\n",
    "\n",
    "# Process data in batches\n",
    "for i in tqdm(range(num_batches), desc=\"Generating translations\"):\n",
    "    \n",
    "    # Define batch boundaries\n",
    "    batch_start = i * batch_size\n",
    "    batch_end = batch_start + batch_size\n",
    "    \n",
    "    # Get current batch (handle last batch which might be smaller)\n",
    "    if i == num_batches - 1:\n",
    "        batch_df = en_to_ar_data.iloc[batch_start:]\n",
    "    else:\n",
    "        batch_df = en_to_ar_data.iloc[batch_start:batch_end]\n",
    "    \n",
    "    # Prepare input prompts (English sentences)\n",
    "    prompts = [f'{english}' for english in batch_df['en']]\n",
    "    \n",
    "    # Generate translations without gradient computation (inference mode)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Set source language for both tokenizers (English input)\n",
    "        tokenizer.src_lang = \"en_XX\"\n",
    "        untrained_tokenizer.src_lang = \"en_XX\"\n",
    "        \n",
    "        # Tokenize input prompts\n",
    "        encodings = tokenizer(\n",
    "            prompts, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=256\n",
    "        )\n",
    "        \n",
    "        # Move inputs to GPU\n",
    "        input_ids = encodings.input_ids.to(device)\n",
    "        attention_mask = encodings.attention_mask.to(device)\n",
    "        \n",
    "        # ğŸ¤– Generate translations with UNTRAINED model\n",
    "        untrained_outputs = untrained_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            forced_bos_token_id=untrained_tokenizer.lang_code_to_id[\"ar_AR\"],  # Force Arabic output\n",
    "            generation_config=GenerationConfig(\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,  # Beam search for better quality\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2  # Prevent repetition\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Decode untrained model outputs\n",
    "        untrained_text_outputs = untrained_tokenizer.batch_decode(\n",
    "            untrained_outputs, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        all_untrained_model_outputs.extend(untrained_text_outputs)\n",
    "        \n",
    "        # ğŸ¯ Generate translations with FINE-TUNED model\n",
    "        trained_outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[\"ar_AR\"],  # Force Arabic output\n",
    "            generation_config=GenerationConfig(\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,  # Beam search for better quality\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2  # Prevent repetition\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Decode fine-tuned model outputs\n",
    "        trained_text_outputs = tokenizer.batch_decode(\n",
    "            trained_outputs, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        all_trained_model_outputs.extend(trained_text_outputs)\n",
    "\n",
    "print(f\"âœ… Generated {len(all_untrained_model_outputs)} untrained translations\")\n",
    "print(f\"âœ… Generated {len(all_trained_model_outputs)} fine-tuned translations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e322f1b-93a8-4666-b955-40a80f7f1676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Translation Results Summary:\n",
      "Total English â†’ Arabic translations: 500\n",
      "\n",
      "ğŸ“ Sample Translation Comparison:\n",
      "\n",
      "Example 1:\n",
      "ğŸ”¤ English Input: What appears from these words is that the nerve exits in the first vertebra from above, and this was invalidated in his words in the first vertebra, and it is apparently invalidated by what he said in his words in the first vertebra.\n",
      "ğŸ¯ Reference Arabic: Ø§Ù„Ø°ÙŠ ÙŠØ¸Ù‡Ø± Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ù… Ø£Ù† Ù…Ø®Ø±Ø¬ Ø§Ù„Ø¹ØµØ¨ ÙÙŠ Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ù† ÙÙˆÙ‚ ÙˆÙ‡Ø°Ø§ Ù‚Ø¯ Ø£Ø¨Ø·Ù„Ù‡ ÙÙŠ ÙƒÙ„Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙˆÙ‡Ùˆ Ø¸Ø§Ù‡Ø± Ø§Ù„Ø¨Ø·Ù„Ø§Ù† Ø¨Ù…Ø§ Ù‚Ø§Ù„Ù‡ ÙÙŠ ÙƒÙ„Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰.\n",
      "ğŸ¤– Untrained Model: Ù…Ø§ ÙŠØ¸Ù‡Ø± Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù‡Ùˆ Ø£Ù† Ø§Ù„Ø§Ø¹ØµØ§Ø¨ ØªØ®Ø±Ø¬ Ù…Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙÙ‚Ø±ÙŠ Ø§Ù„Ø£ÙˆÙ„ Ù…Ù† Ø§Ù„Ø£Ø¹Ù„Ù‰ØŒ ÙˆÙ‚Ø¯ ÙƒØ§Ù† Ù‡Ø°Ø§ ØºÙŠØ± ØµØ§Ù„Ø­Ø§Ù‹ ÙÙŠ ÙƒÙ„Ù…Ø§ØªÙ‡ ÙÙŠ Ø§Ù„Ø¬Ø°Ø¹ Ø§Ù„Ø£ÙˆÙ„ØŒ ÙˆÙ…Ù† Ø§Ù„ÙˆØ§Ø¶Ø­ Ø£Ù†Ù‡ ØºÙŠØ± ØµØ­ÙŠØ­ Ø¨Ù…Ø§ Ù‚Ø§Ù„Ù‡ Ù…Ù† Ø®Ù„Ø§Ù„ ÙƒÙ„Ù…Ø§ØªÙ‡Ø§ ÙÙŠ Ø§Ù„Ø±Ù‘Ù…Ù†Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰.\n",
      "ğŸ”¥ Fine-tuned Model: ÙˆÙ…Ø§ ÙŠØ¸Ù‡Ø± Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù„ÙØ§Ø¸ Ø£Ù† Ø§Ù„Ø¹ØµØ¨ ÙŠØ®Ø±Ø¬ Ù…Ù† Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ù† ÙÙˆÙ‚ØŒ ÙˆÙ‡Ø°Ø§ Ø¨Ø§Ø·Ù„ ÙÙŠ ÙƒÙ„Ø§Ù…Ù‡ Ù…Ù† Ø§Ù„ÙÙ‚Ø§Ø± Ø§Ù„Ø£ÙˆÙ„ØŒ ÙˆÙ‡Ùˆ Ø¸Ø§Ù‡Ø±Ø§ Ø¨Ø§Ø·Ù„ Ø¨Ù…Ø§ Ù‚Ø§Ù„ Ø¨Ù‡ Ù…Ù† Ù‚ÙˆÙ„Ù‡ Ù…Ù† Ø¬Ù…Ù„ØªÙ‡ Ù…Ù† Ø§Ù„Ø¬Ø°Ø¹ Ø§Ù„Ø£ÙˆÙ„.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "ğŸ”¤ English Input: And if it is said: There is no above the surface of the world, nor is there a dimension beyond it, the imagination is unable to accept it, just as if it is said: There is no before the existence of the world, â€œbeforeâ€ is a confirmed existence, it is repelled from accepting it, and just as it is permissible for the imagination to lie in its estimation that above the world is a void, which is a dimension without end, by saying to it: The void is not understood in itself, and as for the dimension, it is dependent on the body whose diameters are far apart, so if the body is finite, the dimension that is dependent on it is finite, and the fullness is cut off and the void is not understood, so it is proven that there is neither void nor fullness beyond the world, even if the imagination does not accept it.\n",
      "ğŸ¯ Reference Arabic: ÙˆØ¥Ø°Ø§ Ù‚ÙŠÙ„: Ù„ÙŠØ³ ÙÙˆÙ‚ Ø³Ø·Ø­ Ø§Ù„Ø¹Ø§Ù„Ù… ÙÙˆÙ‚ ÙˆÙ„Ø§ Ø¨Ø¹Ø¯ Ø£Ø¨Ø¹Ø¯ Ù…Ù†Ù‡ØŒ ÙƒØ§Ø¹ Ø§Ù„ÙˆÙ‡Ù… Ø¹Ù† Ø§Ù„Ø¥Ø°Ø¹Ø§Ù† Ù„Ù‚Ø¨ÙˆÙ„Ù‡ØŒ ÙƒÙ…Ø§ Ø¥Ø°Ø§ Ù‚ÙŠÙ„: Ù„ÙŠØ³ Ù‚Ø¨Ù„ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¹Ø§Ù„Ù… \"Ù‚Ø¨Ù„\" Ù‡Ùˆ ÙˆØ¬ÙˆØ¯ Ù…Ø­Ù‚Ù‚ØŒ Ù†ÙØ± Ø¹Ù† Ù‚Ø¨ÙˆÙ„Ù‡ØŒ ÙˆÙƒÙ…Ø§ Ø¬Ø§Ø² Ø£Ù† ÙŠÙƒØ°Ø¨ Ø§Ù„ÙˆÙ‡Ù… ÙÙŠ ØªÙ‚Ø¯ÙŠØ±Ù‡ ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ù„Ù… Ø®Ù„Ø§Ø¡ Ù‡Ùˆ Ø¨Ø¹Ø¯ Ù„Ø§ Ù†Ù‡Ø§ÙŠØ© Ù„Ù‡ØŒ Ø¨Ø£Ù† ÙŠÙ‚Ø§Ù„ Ù„Ù‡: Ø§Ù„Ø®Ù„Ø§Ø¡ Ù„ÙŠØ³ Ù…ÙÙ‡ÙˆÙ…Ø§ ÙÙŠ Ù†ÙØ³Ù‡ØŒ ÙˆØ£Ù…Ø§ Ø§Ù„Ø¨Ø¹Ø¯ ÙÙ‡Ùˆ ØªØ§Ø¨Ø¹ Ù„Ù„Ø¬Ø³Ù… Ø§Ù„Ø°ÙŠ ØªØªØ¨Ø§Ø¹Ø¯ Ø£Ù‚Ø·Ø§Ø±Ù‡ØŒ ÙØ¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¬Ø³Ù… Ù…ØªÙ†Ø§Ù‡ÙŠØ§ ÙƒØ§Ù† Ø§Ù„Ø¨Ø¹Ø¯ Ø§Ù„Ø°ÙŠ Ù‡Ùˆ ØªØ§Ø¨Ø¹ Ù„Ù‡ Ù…ØªÙ†Ø§Ù‡ÙŠØ§ØŒ ÙˆØ§Ù†Ù‚Ø·Ø¹ Ø§Ù„Ù…Ù„Ø§Ø¡ ÙˆØ§Ù„Ø®Ù„Ø§Ø¡ ØºÙŠØ± Ù…ÙÙ‡ÙˆÙ… ÙØ«Ø¨Øª Ø£Ù†Ù‡ Ù„ÙŠØ³ ÙˆØ±Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù… Ù„Ø§ Ø®Ù„Ø§Ø¡ ÙˆÙ„Ø§ Ù…Ù„Ø§Ø¡ØŒ ÙˆØ¥Ù† ÙƒØ§Ù† Ø§Ù„ÙˆÙ‡Ù… Ù„Ø§ ÙŠØ°Ø¹Ù† Ù„Ù‚Ø¨ÙˆÙ„Ù‡.\n",
      "ğŸ¤– Untrained Model: ÙˆØ¥Ø°Ø§ ÙƒØ§Ù† ÙŠÙ‚ÙˆÙ„: Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙÙˆÙ‚ Ø³Ø·Ø­ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ ÙˆÙ„Ø§ ÙŠÙˆØ¬Ø¯ Ø¨Ø¹Ø¯ ÙˆØ±Ø§Ø¡Ù‡ØŒ ÙØ§Ù„Ø®ÙŠØ§Ù„ Ù„Ø§ ÙŠØ³ØªØ·ÙŠØ¹ Ø£Ù† ÙŠÙ‚Ø¨Ù„Ù‡Ø§ØŒ ØªÙ…Ø§Ù…Ø§Ù‹ ÙƒÙ…Ø§ Ù„Ùˆ Ù‚Ø§Ù„: Ù„ÙŠØ³ Ù‡Ù†Ø§Ùƒ Ù‚Ø¨Ù„ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¹Ø§Ù„Ù… ØŒ \"Ù‚Ø¨Ù„\" Ù‡Ùˆ ÙˆØ¬ÙˆØ¯ Ù…Ø¤ÙƒØ¯ØŒ ÙŠØªÙ… Ù…Ù†Ø¹Ù‡ Ù…Ù† Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ø¨Ù‡ØŒ ÙˆÙƒÙ…Ø§ Ù‡Ùˆ Ù…Ø³Ù…ÙˆØ­ Ù„Ù„Ø®ÙŠØ§Ù„ÙŠ Ø£Ù† ÙŠÙƒÙˆÙ† ÙÙŠ ØªÙ‚Ø¯ÙŠØ±Ù‡ Ø£Ù† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ù„Ù… ÙŠÙˆØ¬Ø¯ ÙØ±Ø§ØºØŒ ÙˆÙ‡Ùˆ Ø¨Ø¹Ø¯ Ø¨Ù„Ø§ Ù†Ù‡Ø§ÙŠØ©ØŒ Ø¨Ù‚ÙˆÙ„Ù‡ Ù„Ù‡: Ø§Ù„ÙØ±Ø§Øº ØºÙŠØ± Ù…ÙÙ‡ÙˆÙ… ÙÙŠ Ø­Ø¯ Ø°Ø§ØªÙ‡ ØŒ ÙˆØ¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø¨Ø¹Ø¯ØŒ ÙÙ‡Ùˆ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø³Ù… Ø§Ù„Ø°ÙŠ ÙŠØ¨Ø¹Ø¯ Ù‚Ø·Ø±Ù‡ Ø¨Ø¹ÙŠØ¯Ø§Ù‹ØŒ Ù„Ø°Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¬Ø³Ù… Ù…Ø­Ø¯ÙˆØ¯Ø§Ù‹ ØŒ ÙØ§Ù„Ø¨Ø¹Ø¯ Ø§Ù„Ø°ÙŠ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„ÙŠÙ‡ Ù‡Ùˆ Ù…Ø­Ø¯ÙˆØ¯ØŒ ÙˆØ§Ù„Ø§Ù…ØªÙ„Ø§Ø¡ ÙŠØªÙ… Ù‚Ø·Ø¹Ù‡ ÙˆØ§Ù„ÙØ±Ø§Øº Ù„Ø§ ÙŠÙÙ‡Ù…ØŒ Ù„Ø°Ù„Ùƒ ÙŠØ«Ø¨Øª Ø£Ù†Ù‡ Ù„ÙŠØ³ Ù‡Ù†Ø§Ù„Ùƒ ÙØ¶Ø§Ø¡ Ø£Ùˆ Ù…Ù„Ø§Ø¡Ø© ÙˆØ±Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù… Ø­ØªÙ‰ Ù„Ùˆ Ù„Ù… ÙŠØªØ­Ù…Ù„Ù‡ Ø§Ù„Ø®ÙŠØ§Ù„.\n",
      "ğŸ”¥ Fine-tuned Model: ÙˆØ¥Ø°Ø§ Ù‚ÙŠÙ„: Ù„ÙŠØ³ ÙÙˆÙ‚ Ø³Ø·Ø­ Ø§Ù„Ø¹Ø§Ù„Ù… Ø¨Ø¹Ø¯ ÙˆÙ„Ø§ ÙˆØ±Ø§Ø¡Ù‡ Ø¨Ø¹Ø¯ ÙØ¥Ù† Ø§Ù„ÙˆÙ‡Ù… Ù„Ø§ ÙŠÙ‚Ø¯Ø± Ø¹Ù„Ù‰ Ù‚Ø¨ÙˆÙ„Ù‡ØŒ ÙƒÙ…Ø§ Ø£Ù† ÙŠÙ‚Ø§Ù„: Ù„Ø§ Ù‚Ø¨Ù„ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ \"Ù‚Ø¨Ù„\" Ù‡Ùˆ ÙˆØ¬ÙˆØ¯ Ù…ÙˆØ«Ù‚ØŒ ÙŠÙ†Ø¯ÙØ¹ Ø¹Ù†Ù‡ Ø¹Ù† Ù‚Ø¨ÙˆÙ„Ù‡Ø§ØŒ ÙˆÙƒÙ…Ø§ ÙŠØ­Ù„ Ø£Ù† ÙŠÙ‚Ø¹ ÙÙŠ Ø§Ù„ØªØ®ÙŠÙ„ ÙÙŠ ØªÙ‚Ø¯ÙŠØ±Ù‡ Ø£Ù† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ù„Ù… Ø®Ù„Ø§Ø¡ØŒ ÙˆÙ‡Ùˆ Ø¨Ø¹Ø¯ Ù„Ø§ Ù†Ù‡Ø§ÙŠØ© Ù„Ù‡ØŒ Ø¨Ù‚ÙˆÙ„Ù‡ Ù„Ù‡: Ø§Ù„ÙØ¶ÙˆÙ„ Ù„Ø§ ØªØ¹Ù‚Ù„ ÙÙŠ Ù†ÙØ³Ù‡ØŒ ÙˆØ£Ù…Ø§ Ø§Ù„Ø¨Ø¹Ø¯ ÙÙ‡Ùˆ Ù…Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø³Ù… Ø§Ù„Ø°ÙŠ ØªØ¨Ø¹Ø¯ Ù‚Ø·Ø±Ù‡ ÙØ¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¬Ø³Ù… Ù…Ù‚ØªØµØ±Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯ Ø¹Ù„ÙŠÙ‡ Ù…Ù‚Ø¯Ø±Ø§ØŒ ÙˆØ§Ù„Ù„Ù„Ø§Ø¡ Ù…Ù‚Ø·ÙˆØ¹Ø§ ÙˆØ§Ù„Ù…Ø§Ø¡ ØºÙŠØ± Ù…Ø¯Ø±ÙƒØŒ ÙÙŠØ«Ø¨Øª Ø£Ù†Ù‡ Ù„Ø§ ÙŠÙƒÙˆÙ† ÙˆØ±Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù… ÙØ¶Ù„Ø§ ÙˆÙ„Ø§ Ù…Ù„Ø§Ø¡ ÙˆØ¥Ù† Ù„Ù… ÙŠÙ‚Ø¨Ù„Ù‡ Ø§Ù„ÙˆÙ‡Ù…Ø§.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "ğŸ”¤ English Input: And God is the Grantor of success. The second research: The benefits of the science of anatomy. The doctorâ€™s benefit from this science is partly in knowledge, partly in action, and partly in reasoning.\n",
      "ğŸ¯ Reference Arabic: ÙˆØ§Ù„Ù„Ù‡ ÙˆÙ„ÙŠ Ø§Ù„ØªÙˆÙÙŠÙ‚ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø«Ø§Ù†ÙŠ ÙÙˆØ§Ø¦Ø¯ Ø¹Ù„Ù… Ø§Ù„ØªØ´Ø±ÙŠØ­ Ø§Ù†ØªÙØ§Ø¹ Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù„Ù… Ø¨Ø¹Ø¶Ù‡ ÙÙŠ Ø§Ù„Ø¹Ù„Ù…ØŒ ÙˆØ¨Ø¹Ø¶Ù‡ ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ØŒ ÙˆØ¨Ø¹Ø¶Ù‡ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.\n",
      "ğŸ¤– Untrained Model: Ùˆ Ø§Ù„Ø±Ø¨ Ù‡Ùˆ Ø§Ù„Ù…ÙØ¹Ø·ÙÙ„ Ù„Ù„Ù†Ø¬Ø§Ø­. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø«Ø§Ù†ÙŠ: ÙÙˆØ§Ø¦Ø¯ Ø¹Ù„Ù… Ø§Ù„ØªØ´Ø±ÙŠØ­. ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù„Ù… Ù‡ÙŠ Ø¬Ø²Ø¦ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ Ø¬Ø²Ø¦ÙŠÙ‹Ø§ ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ØŒ ÙˆØ¬Ø²Ø£Ù‹ Ù…Ø§ ÙÙŠ Ø§Ù„ØªÙØ³ÙŠØ±.\n",
      "ğŸ”¥ Fine-tuned Model: ÙˆØ§Ù„Ù„Ù‡ Ù…Ø¤Ù‡Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ØŒ ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ø«Ø§Ù†ÙŠ: Ù…Ù†Ø§ÙØ¹ Ø¹Ù„Ù… Ø§Ù„ØªØ´Ø±ÙŠØ­ØŒ ÙˆÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù„Ù… Ù‡ÙŠ Ø¬Ø²Ø¦ÙŠØ§ ÙÙŠ Ø§Ù„Ø¹Ù„Ù… ÙˆØ¬Ø²Ø¡ Ù…Ù† Ø§Ù„ÙØ¹Ù„ØŒ ÙˆÙ…Ù‚Ø¯Ø§Ø± Ø§Ù„Ø¹Ù„Ø©.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Verify we have the correct number of translations\n",
    "assert len(all_untrained_model_outputs) == len(en_to_ar_data), \"Mismatch in untrained outputs\"\n",
    "assert len(all_trained_model_outputs) == len(en_to_ar_data), \"Mismatch in trained outputs\"\n",
    "\n",
    "# Store translations in the dataframe for easy comparison\n",
    "en_to_ar_data = en_to_ar_data.copy()  # Avoid SettingWithCopyWarning\n",
    "en_to_ar_data['trained_model_arabic'] = all_trained_model_outputs\n",
    "en_to_ar_data['untrained_model_arabic'] = all_untrained_model_outputs\n",
    "\n",
    "print(\"ğŸ“Š Translation Results Summary:\")\n",
    "print(f\"Total English â†’ Arabic translations: {len(en_to_ar_data)}\")\n",
    "\n",
    "# Display a few examples for qualitative assessment\n",
    "print(\"\\nğŸ“ Sample Translation Comparison:\")\n",
    "for i in range(3):\n",
    "    sample = en_to_ar_data.iloc[i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"ğŸ”¤ English Input: {sample['en']}\")\n",
    "    print(f\"ğŸ¯ Reference Arabic: {sample['ar']}\")\n",
    "    print(f\"ğŸ¤– Untrained Model: {sample['untrained_model_arabic']}\")\n",
    "    print(f\"ğŸ”¥ Fine-tuned Model: {sample['trained_model_arabic']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfabf08d-5cdd-425a-9ab2-c67e21c7188a",
   "metadata": {},
   "source": [
    "## ğŸ“Š BLEU Score Computation\n",
    "\n",
    "Computing BLEU scores to quantitatively compare translation quality:\n",
    "- **BLEU-1, BLEU-2, BLEU-3, BLEU-4**: Different n-gram precision scores\n",
    "- **Higher scores**: Better translation quality\n",
    "- **Comparison**: Untrained vs. Fine-tuned model performance\n",
    "\n",
    "### BLEU Score Interpretation:\n",
    "- **0-10**: Very poor quality\n",
    "- **10-20**: Poor quality  \n",
    "- **20-30**: Reasonable quality\n",
    "- **30-40**: Good quality\n",
    "- **40+**: Excellent quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48df49df-a355-49d0-b505-b23ad7dba79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Computing BLEU scores for English â†’ Arabic translation...\n",
      "ğŸ“ Computing BLEU for 500 translation pairs...\n",
      "âœ… BLEU computation completed!\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for BLEU evaluation\n",
    "print(\"ğŸ“Š Computing BLEU scores for English â†’ Arabic translation...\")\n",
    "\n",
    "# Extract predictions and references\n",
    "predictions_untrained = en_to_ar_data['untrained_model_arabic'].tolist()\n",
    "predictions_trained = en_to_ar_data['trained_model_arabic'].tolist()\n",
    "references_raw = en_to_ar_data['ar'].tolist()\n",
    "\n",
    "# Format references for BLEU computation (each reference needs to be in a list)\n",
    "references = [[ref] for ref in references_raw]\n",
    "\n",
    "# Ensure all data is in string format (required by BLEU metric)\n",
    "predictions_untrained = [str(pred) for pred in predictions_untrained]\n",
    "predictions_trained = [str(pred) for pred in predictions_trained]\n",
    "references = [[str(ref)] for ref in references_raw]\n",
    "\n",
    "print(f\"ğŸ“ Computing BLEU for {len(predictions_untrained)} translation pairs...\")\n",
    "\n",
    "# Compute BLEU scores for untrained model\n",
    "original_bleu = bleu.compute(\n",
    "    predictions=predictions_untrained,\n",
    "    references=references,\n",
    ")\n",
    "\n",
    "# Compute BLEU scores for fine-tuned model\n",
    "trained_bleu = bleu.compute(\n",
    "    predictions=predictions_trained,\n",
    "    references=references,\n",
    ")\n",
    "\n",
    "print(\"âœ… BLEU computation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48b75b-e006-42ae-b328-eab8710be27c",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ English â†’ Arabic BLEU Results\n",
    "\n",
    "Comparing the translation quality between pre-trained and fine-tuned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc405d66-ed06-4ac2-b475-e4e71ef05526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ English â†’ Arabic Translation Results\n",
      "==================================================\n",
      "\n",
      "ğŸ¤– Original (Pre-trained) Model BLEU Scores:\n",
      "{\n",
      "  \"bleu\": 0.04095130155220747,\n",
      "  \"precisions\": [\n",
      "    0.27650206597575017,\n",
      "    0.07137348383930449,\n",
      "    0.02172491462617162,\n",
      "    0.006559601900022619\n",
      "  ],\n",
      "  \"brevity_penalty\": 1.0,\n",
      "  \"length_ratio\": 1.129706152433425,\n",
      "  \"translation_length\": 14763,\n",
      "  \"reference_length\": 13068\n",
      "}\n",
      "\n",
      "ğŸ”¥ Fine-tuned Model BLEU Scores:\n",
      "{\n",
      "  \"bleu\": 0.1251646739834074,\n",
      "  \"precisions\": [\n",
      "    0.4462803320561941,\n",
      "    0.18548387096774194,\n",
      "    0.08501040943789036,\n",
      "    0.04143997098295248\n",
      "  ],\n",
      "  \"brevity_penalty\": 0.9578123009106891,\n",
      "  \"length_ratio\": 0.9586776859504132,\n",
      "  \"translation_length\": 12528,\n",
      "  \"reference_length\": 13068\n",
      "}\n",
      "\n",
      "ğŸ“ˆ BLEU Score Improvement:\n",
      "Original BLEU: 0.0410\n",
      "Fine-tuned BLEU: 0.1252\n",
      "Improvement: +0.0842 (205.6%)\n",
      "\n",
      "ğŸ“Š Detailed N-gram Improvements:\n",
      "BLEU-1: 0.2765 â†’ 0.4463 (+0.1698)\n",
      "BLEU-2: 0.0714 â†’ 0.1855 (+0.1141)\n",
      "BLEU-3: 0.0217 â†’ 0.0850 (+0.0633)\n",
      "BLEU-4: 0.0066 â†’ 0.0414 (+0.0349)\n"
     ]
    }
   ],
   "source": [
    "print('ğŸ”„ English â†’ Arabic Translation Results')\n",
    "print('=' * 50)\n",
    "\n",
    "print('\\nğŸ¤– Original (Pre-trained) Model BLEU Scores:')\n",
    "print(json.dumps(original_bleu, indent=2))\n",
    "\n",
    "print('\\nğŸ”¥ Fine-tuned Model BLEU Scores:')\n",
    "print(json.dumps(trained_bleu, indent=2))\n",
    "\n",
    "# Calculate improvement\n",
    "bleu_improvement = trained_bleu['bleu'] - original_bleu['bleu']\n",
    "print(f'\\nğŸ“ˆ BLEU Score Improvement:')\n",
    "print(f'Original BLEU: {original_bleu[\"bleu\"]:.4f}')\n",
    "print(f'Fine-tuned BLEU: {trained_bleu[\"bleu\"]:.4f}')\n",
    "print(f'Improvement: +{bleu_improvement:.4f} ({bleu_improvement/original_bleu[\"bleu\"]*100:.1f}%)')\n",
    "\n",
    "# Individual n-gram improvements\n",
    "print(f'\\nğŸ“Š Detailed N-gram Improvements:')\n",
    "for i, (orig, trained) in enumerate(zip(original_bleu['precisions'], trained_bleu['precisions'])):\n",
    "    improvement = trained - orig\n",
    "    print(f'BLEU-{i+1}: {orig:.4f} â†’ {trained:.4f} (+{improvement:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3caf33-32ee-44a0-b400-6aae80cebabb",
   "metadata": {},
   "source": [
    "## ğŸ”„ Arabic â†’ English Translation Evaluation\n",
    "\n",
    "Now evaluating the reverse direction to assess bidirectional translation capabilities:\n",
    "- **Input**: Arabic sentences from our test set\n",
    "- **Output**: English translations\n",
    "- **Comparison**: Both models' English generation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94ab16f8-0c10-41d0-bdcc-dbe73a5115bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing 500 Arabic â†’ English translations...\n",
      "Using 32 batches of size 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Arabicâ†’English: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [04:02<00:00,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 500 untrained Arabicâ†’English translations\n",
      "âœ… Generated 500 fine-tuned Arabicâ†’English translations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Reset output lists for Arabic â†’ English evaluation\n",
    "all_untrained_ar_to_en = []\n",
    "all_trained_ar_to_en = []\n",
    "\n",
    "# Calculate batches for Arabic â†’ English data\n",
    "ar_batch_size = 16\n",
    "ar_num_batches = int(math.ceil(len(ar_to_en_data) / ar_batch_size))\n",
    "\n",
    "print(f\"ğŸ”„ Processing {len(ar_to_en_data)} Arabic â†’ English translations...\")\n",
    "print(f\"Using {ar_num_batches} batches of size {ar_batch_size}\")\n",
    "\n",
    "# Process Arabic â†’ English translations in batches\n",
    "for i in tqdm(range(ar_num_batches), desc=\"Generating Arabicâ†’English\"):\n",
    "    \n",
    "    # Define batch boundaries\n",
    "    batch_start = i * ar_batch_size\n",
    "    batch_end = batch_start + ar_batch_size\n",
    "    \n",
    "    # Get current batch\n",
    "    if i == ar_num_batches - 1:\n",
    "        batch_df = ar_to_en_data.iloc[batch_start:]\n",
    "    else:\n",
    "        batch_df = ar_to_en_data.iloc[batch_start:batch_end]\n",
    "    \n",
    "    # Prepare Arabic input prompts\n",
    "    arabic_prompts = [f'{arabic}' for arabic in batch_df['ar']]\n",
    "    \n",
    "    # Generate translations without gradient computation\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Set source language to Arabic for both tokenizers\n",
    "        tokenizer.src_lang = \"ar_AR\"\n",
    "        untrained_tokenizer.src_lang = \"ar_AR\"\n",
    "        \n",
    "        # Tokenize Arabic input\n",
    "        encodings = tokenizer(\n",
    "            arabic_prompts, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=256\n",
    "        )\n",
    "        \n",
    "        # Move to GPU\n",
    "        input_ids = encodings.input_ids.to(device)\n",
    "        attention_mask = encodings.attention_mask.to(device)\n",
    "        \n",
    "        # Generate English translations with UNTRAINED model\n",
    "        untrained_ar_outputs = untrained_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            forced_bos_token_id=untrained_tokenizer.lang_code_to_id[\"en_XX\"],  # Force English output\n",
    "            generation_config=GenerationConfig(\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Decode untrained model outputs\n",
    "        untrained_ar_text = untrained_tokenizer.batch_decode(\n",
    "            untrained_ar_outputs, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        all_untrained_ar_to_en.extend(untrained_ar_text)\n",
    "        \n",
    "        # Generate English translations with FINE-TUNED model\n",
    "        trained_ar_outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"],  # Force English output\n",
    "            generation_config=GenerationConfig(\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Decode fine-tuned model outputs\n",
    "        trained_ar_text = tokenizer.batch_decode(\n",
    "            trained_ar_outputs, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        all_trained_ar_to_en.extend(trained_ar_text)\n",
    "\n",
    "print(f\"âœ… Generated {len(all_untrained_ar_to_en)} untrained Arabicâ†’English translations\")\n",
    "print(f\"âœ… Generated {len(all_trained_ar_to_en)} fine-tuned Arabicâ†’English translations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e967ac4-d1e7-4849-b07a-367f73ba6dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Computing BLEU scores for Arabic â†’ English translation...\n",
      "âœ… Arabic â†’ English BLEU computation completed!\n"
     ]
    }
   ],
   "source": [
    "# Store Arabic â†’ English translations\n",
    "ar_to_en_data = ar_to_en_data.copy()\n",
    "ar_to_en_data['trained_model_english'] = all_trained_ar_to_en\n",
    "ar_to_en_data['untrained_model_english'] = all_untrained_ar_to_en\n",
    "\n",
    "# Prepare data for BLEU evaluation (Arabic â†’ English)\n",
    "print(\"ğŸ“Š Computing BLEU scores for Arabic â†’ English translation...\")\n",
    "\n",
    "ar_predictions_untrained = ar_to_en_data['untrained_model_english'].tolist()\n",
    "ar_predictions_trained = ar_to_en_data['trained_model_english'].tolist()\n",
    "ar_references_raw = ar_to_en_data['en'].tolist()\n",
    "\n",
    "# Format for BLEU computation\n",
    "ar_references = [[ref] for ref in ar_references_raw]\n",
    "ar_predictions_untrained = [str(pred) for pred in ar_predictions_untrained]\n",
    "ar_predictions_trained = [str(pred) for pred in ar_predictions_trained]\n",
    "ar_references = [[str(ref)] for ref in ar_references_raw]\n",
    "\n",
    "# Compute BLEU scores\n",
    "ar_original_bleu = bleu.compute(\n",
    "    predictions=ar_predictions_untrained,\n",
    "    references=ar_references,\n",
    ")\n",
    "\n",
    "ar_trained_bleu = bleu.compute(\n",
    "    predictions=ar_predictions_trained,\n",
    "    references=ar_references,\n",
    ")\n",
    "\n",
    "print(\"âœ… Arabic â†’ English BLEU computation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f88613-1895-410f-810d-546f946f1154",
   "metadata": {},
   "source": [
    "## ğŸ¯ Complete Bidirectional Translation Results\n",
    "\n",
    "Final comparison of our fine-tuning results across both translation directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "685e2623-635f-4556-9be3-bcf86d427782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ COMPLETE BIDIRECTIONAL TRANSLATION EVALUATION\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ENGLISH â†’ ARABIC RESULTS:\n",
      "------------------------------\n",
      "ğŸ¤– Original Model:\n",
      "{\n",
      "  \"bleu\": 0.04095130155220747,\n",
      "  \"precisions\": [\n",
      "    0.27650206597575017,\n",
      "    0.07137348383930449,\n",
      "    0.02172491462617162,\n",
      "    0.006559601900022619\n",
      "  ],\n",
      "  \"brevity_penalty\": 1.0,\n",
      "  \"length_ratio\": 1.129706152433425,\n",
      "  \"translation_length\": 14763,\n",
      "  \"reference_length\": 13068\n",
      "}\n",
      "\n",
      "ğŸ”¥ Fine-tuned Model:\n",
      "{\n",
      "  \"bleu\": 0.1251646739834074,\n",
      "  \"precisions\": [\n",
      "    0.4462803320561941,\n",
      "    0.18548387096774194,\n",
      "    0.08501040943789036,\n",
      "    0.04143997098295248\n",
      "  ],\n",
      "  \"brevity_penalty\": 0.9578123009106891,\n",
      "  \"length_ratio\": 0.9586776859504132,\n",
      "  \"translation_length\": 12528,\n",
      "  \"reference_length\": 13068\n",
      "}\n",
      "\n",
      "ğŸ“ˆ ENâ†’AR Improvement: +0.0842 (205.6%)\n",
      "\n",
      "ğŸ“Š ARABIC â†’ ENGLISH RESULTS:\n",
      "------------------------------\n",
      "ğŸ¤– Original Model:\n",
      "{\n",
      "  \"bleu\": 0.0017288444268053257,\n",
      "  \"precisions\": [\n",
      "    0.13921317767471614,\n",
      "    0.004247876061969015,\n",
      "    0.0007822005909960021,\n",
      "    0.0002725538293813028\n",
      "  ],\n",
      "  \"brevity_penalty\": 0.5159412616096217,\n",
      "  \"length_ratio\": 0.6017707631604273,\n",
      "  \"translation_length\": 12506,\n",
      "  \"reference_length\": 20782\n",
      "}\n",
      "\n",
      "ğŸ”¥ Fine-tuned Model:\n",
      "{\n",
      "  \"bleu\": 0.15442453834231745,\n",
      "  \"precisions\": [\n",
      "    0.5233875653689716,\n",
      "    0.21700079396585947,\n",
      "    0.09993893751272136,\n",
      "    0.05137844611528822\n",
      "  ],\n",
      "  \"brevity_penalty\": 0.9937249808329195,\n",
      "  \"length_ratio\": 0.993744586661534,\n",
      "  \"translation_length\": 20652,\n",
      "  \"reference_length\": 20782\n",
      "}\n",
      "\n",
      "ğŸ“ˆ ARâ†’EN Improvement: +0.1527 (8832.2%)\n",
      "\n",
      "ğŸ† OVERALL SUMMARY:\n",
      "==============================\n",
      "ğŸ“Š Average Original BLEU: 0.0213\n",
      "ğŸ”¥ Average Fine-tuned BLEU: 0.1398\n",
      "ğŸ“ˆ Average Improvement: +0.1185\n",
      "\n",
      "ğŸ“ SAMPLE TRANSLATION COMPARISON:\n",
      "========================================\n",
      "\n",
      "ğŸ”¤ English â†’ Arabic Example:\n",
      "Input: What appears from these words is that the nerve exits in the first vertebra from above, and this was invalidated in his words in the first vertebra, and it is apparently invalidated by what he said in his words in the first vertebra.\n",
      "Reference: Ø§Ù„Ø°ÙŠ ÙŠØ¸Ù‡Ø± Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ù… Ø£Ù† Ù…Ø®Ø±Ø¬ Ø§Ù„Ø¹ØµØ¨ ÙÙŠ Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ù† ÙÙˆÙ‚ ÙˆÙ‡Ø°Ø§ Ù‚Ø¯ Ø£Ø¨Ø·Ù„Ù‡ ÙÙŠ ÙƒÙ„Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙˆÙ‡Ùˆ Ø¸Ø§Ù‡Ø± Ø§Ù„Ø¨Ø·Ù„Ø§Ù† Ø¨Ù…Ø§ Ù‚Ø§Ù„Ù‡ ÙÙŠ ÙƒÙ„Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰.\n",
      "Pre-trained: Ù…Ø§ ÙŠØ¸Ù‡Ø± Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù‡Ùˆ Ø£Ù† Ø§Ù„Ø§Ø¹ØµØ§Ø¨ ØªØ®Ø±Ø¬ Ù…Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙÙ‚Ø±ÙŠ Ø§Ù„Ø£ÙˆÙ„ Ù…Ù† Ø§Ù„Ø£Ø¹Ù„Ù‰ØŒ ÙˆÙ‚Ø¯ ÙƒØ§Ù† Ù‡Ø°Ø§ ØºÙŠØ± ØµØ§Ù„Ø­Ø§Ù‹ ÙÙŠ ÙƒÙ„Ù…Ø§ØªÙ‡ ÙÙŠ Ø§Ù„Ø¬Ø°Ø¹ Ø§Ù„Ø£ÙˆÙ„ØŒ ÙˆÙ…Ù† Ø§Ù„ÙˆØ§Ø¶Ø­ Ø£Ù†Ù‡ ØºÙŠØ± ØµØ­ÙŠØ­ Ø¨Ù…Ø§ Ù‚Ø§Ù„Ù‡ Ù…Ù† Ø®Ù„Ø§Ù„ ÙƒÙ„Ù…Ø§ØªÙ‡Ø§ ÙÙŠ Ø§Ù„Ø±Ù‘Ù…Ù†Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰.\n",
      "Fine-tuned: ÙˆÙ…Ø§ ÙŠØ¸Ù‡Ø± Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù„ÙØ§Ø¸ Ø£Ù† Ø§Ù„Ø¹ØµØ¨ ÙŠØ®Ø±Ø¬ Ù…Ù† Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ù† ÙÙˆÙ‚ØŒ ÙˆÙ‡Ø°Ø§ Ø¨Ø§Ø·Ù„ ÙÙŠ ÙƒÙ„Ø§Ù…Ù‡ Ù…Ù† Ø§Ù„ÙÙ‚Ø§Ø± Ø§Ù„Ø£ÙˆÙ„ØŒ ÙˆÙ‡Ùˆ Ø¸Ø§Ù‡Ø±Ø§ Ø¨Ø§Ø·Ù„ Ø¨Ù…Ø§ Ù‚Ø§Ù„ Ø¨Ù‡ Ù…Ù† Ù‚ÙˆÙ„Ù‡ Ù…Ù† Ø¬Ù…Ù„ØªÙ‡ Ù…Ù† Ø§Ù„Ø¬Ø°Ø¹ Ø§Ù„Ø£ÙˆÙ„.\n",
      "\n",
      "ğŸ”¤ Arabic â†’ English Example:\n",
      "Input: [ÙƒÙŠÙÙŠØ© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ùƒ] ÙˆØ¥Ù† ÙƒØ§Ù† Ø¯Ø®ÙˆÙ„Ù‡ Ø¹Ù„ÙŠÙ‡ Ù…Ù† Ø§Ù„Ø¨Ø§Ø¨ Ø§Ù„Ø£ÙˆÙ„ Ø§Ù„Ø°ÙŠ ÙŠÙ‚Ø§Ø¨Ù„ ÙˆØ¬Ù‡ Ø§Ù„Ù…Ù„Ùƒ ÙˆÙŠØ­Ø§Ø°ÙŠÙ‡ - ÙˆÙƒØ§Ù† Ù„Ù‡ Ø·Ø±ÙŠÙ‚ Ø¹Ù† ÙŠÙ…ÙŠÙ†Ù‡ Ø£Ùˆ Ø´Ù…Ø§Ù„Ù‡ - Ø¹Ø¯Ù„ Ù†Ø­Ùˆ Ø§Ù„Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø°ÙŠ Ù„Ø§ ÙŠÙ‚Ø§Ø¨Ù„Ù‡ ÙÙŠÙ‡ Ø¨ÙˆØ¬Ù‡Ù‡ØŒ Ø«Ù… Ø§Ù†Ø­Ø±Ù Ù†Ø­Ùˆ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ù„ÙƒØŒ ÙØ³Ù„Ù… Ù‚Ø§Ø¦Ù…Ø§ Ù…Ù„Ø§Ø­Ø¸Ø§ Ù„Ù„Ù…Ù„Ùƒ.\n",
      "Reference: [How to enter upon the king] If he enters upon him from the first door that faces the kingâ€™s face and is parallel to him - and he has a path to his right or left - he turns towards the path that does not face him, then turns towards the kingâ€™s seat, and greets him while standing and observing the king.\n",
      "Pre-trained: [ÙƒÙŠÙÙŠØ© Ø§Ù„Ø¯Ø®ÙˆÙ„ on Ø§Ù„Ù…Ù„Ùƒ] ÙˆØ¥Ù† ÙƒØ§Ù† Ø¯Ø®ÙˆÙ„Ù‡ Ø¹Ù„ÙŠÙ‡ Ù…Ù† Ø§Ù„Ø¨Ø§Ø¨ Ø§Ù„Ø£ÙˆÙ„ Ø§Ù„Ø°ÙŠ ÙŠÙ‚Ø§Ø¨Ù„ ÙˆØ¬Ù‡ Ø§Ù„Ù…Ù„Ùƒ andÙŠØ­Ø§Ø°ÙŠÙ‡ - ÙˆÙƒØ§Ù† Ù„Ù‡ Ø·Ø±ÙŠÙ‚ Ø¹Ù† ÙŠÙ…ÙŠÙ†Ù‡ Ø£Ùˆ Ø´Ù…Ø§Ù„Ù‡ - Ø¹Ø¯Ù„ Ù†Ø­Ùˆ Ø§Ù„Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø°ÙŠ Ù„Ø§ ÙŠ Ù‚Ø§Ø¨Ù„Ù‡ ÙÙŠÙ‡ Ø¨ÙˆØ¬Ù‡Ù‡, Ø«Ù… Ø§Ù†Ø­Ø±Ù Ù†Ø­Ùˆ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ù„Ùƒ, ÙØ³Ù„Ù… Ù‚Ø§Ø¦Ù…Ø§ Ù…Ù„Ø§Ø­Ø¸Ø§ Ù„Ù„Ù…Ù„Ùƒ.\n",
      "Fine-tuned: [The manner of entering on the king] If it is from the first door that enters it, and that meets the Kingâ€™s point and his permission - and it has a way from his sin or his north - it should be towards the way that does not meet it by side, then it turns to the royal congress, so the permission is given in a state of control for the master.\n"
     ]
    }
   ],
   "source": [
    "print('ğŸŒ COMPLETE BIDIRECTIONAL TRANSLATION EVALUATION')\n",
    "print('=' * 60)\n",
    "\n",
    "print('\\nğŸ“Š ENGLISH â†’ ARABIC RESULTS:')\n",
    "print('-' * 30)\n",
    "print('ğŸ¤– Original Model:')\n",
    "print(json.dumps(original_bleu, indent=2))\n",
    "print('\\nğŸ”¥ Fine-tuned Model:')\n",
    "print(json.dumps(trained_bleu, indent=2))\n",
    "\n",
    "en_ar_improvement = trained_bleu['bleu'] - original_bleu['bleu']\n",
    "print(f'\\nğŸ“ˆ ENâ†’AR Improvement: +{en_ar_improvement:.4f} ({en_ar_improvement/original_bleu[\"bleu\"]*100:.1f}%)')\n",
    "\n",
    "print('\\nğŸ“Š ARABIC â†’ ENGLISH RESULTS:')\n",
    "print('-' * 30)\n",
    "print('ğŸ¤– Original Model:')\n",
    "print(json.dumps(ar_original_bleu, indent=2))\n",
    "print('\\nğŸ”¥ Fine-tuned Model:')\n",
    "print(json.dumps(ar_trained_bleu, indent=2))\n",
    "\n",
    "ar_en_improvement = ar_trained_bleu['bleu'] - ar_original_bleu['bleu']\n",
    "print(f'\\nğŸ“ˆ ARâ†’EN Improvement: +{ar_en_improvement:.4f} ({ar_en_improvement/ar_original_bleu[\"bleu\"]*100:.1f}%)')\n",
    "\n",
    "print('\\nğŸ† OVERALL SUMMARY:')\n",
    "print('=' * 30)\n",
    "print(f'ğŸ“Š Average Original BLEU: {(original_bleu[\"bleu\"] + ar_original_bleu[\"bleu\"])/2:.4f}')\n",
    "print(f'ğŸ”¥ Average Fine-tuned BLEU: {(trained_bleu[\"bleu\"] + ar_trained_bleu[\"bleu\"])/2:.4f}')\n",
    "avg_improvement = (en_ar_improvement + ar_en_improvement) / 2\n",
    "print(f'ğŸ“ˆ Average Improvement: +{avg_improvement:.4f}')\n",
    "\n",
    "# Qualitative examples\n",
    "print('\\nğŸ“ SAMPLE TRANSLATION COMPARISON:')\n",
    "print('=' * 40)\n",
    "\n",
    "print('\\nğŸ”¤ English â†’ Arabic Example:')\n",
    "sample_en = en_to_ar_data.iloc[0]\n",
    "print(f\"Input: {sample_en['en']}\")\n",
    "print(f\"Reference: {sample_en['ar']}\")\n",
    "print(f\"Pre-trained: {sample_en['untrained_model_arabic']}\")\n",
    "print(f\"Fine-tuned: {sample_en['trained_model_arabic']}\")\n",
    "\n",
    "print('\\nğŸ”¤ Arabic â†’ English Example:')\n",
    "sample_ar = ar_to_en_data.iloc[0]\n",
    "print(f\"Input: {sample_ar['ar']}\")\n",
    "print(f\"Reference: {sample_ar['en']}\")\n",
    "print(f\"Pre-trained: {sample_ar['untrained_model_english']}\")\n",
    "print(f\"Fine-tuned: {sample_ar['trained_model_english']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbc950-e143-4a7d-9b82-8219252e75c7",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Exercise: Advanced Fine-tuning Experiments\n",
    "\n",
    "### ğŸ“ Suggested Improvements to Try:\n",
    "\n",
    "1. **Increase Training Data**: \n",
    "   - Use the full dataset (not just 5K samples)\n",
    "   - Observe how performance scales with data size\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Try different learning rates (1e-5, 2e-5, 1e-4)\n",
    "   - Experiment with different batch sizes\n",
    "   - Adjust the number of training epochs\n",
    "\n",
    "3. **Advanced Generation Parameters**:\n",
    "   - Experiment with different beam sizes (2, 6, 8)\n",
    "   - Try temperature sampling vs beam search\n",
    "   - Test different max_length values\n",
    "\n",
    "4. **Data Augmentation**:\n",
    "   - Add noise to input text\n",
    "   - Try back-translation augmentation\n",
    "   - Experiment with different prompting strategies\n",
    "\n",
    "5. **Evaluation Metrics**:\n",
    "   - Implement ROUGE scores\n",
    "   - Add METEOR evaluation\n",
    "   - Try BERTScore for semantic similarity\n",
    "\n",
    "### ğŸ“Š Questions to Explore:\n",
    "- How does the model perform on longer sentences?\n",
    "- What types of sentences show the most improvement?\n",
    "- Does the model maintain consistency across different domains?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995f4e4-6470-4f29-a9a3-9f96af5b9764",
   "metadata": {},
   "source": [
    "### Contributed by: Ali Habibullah"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
