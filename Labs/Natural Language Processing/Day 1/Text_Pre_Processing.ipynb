{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xYRT3l1kPaY"
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk0NSD46xTaj"
      },
      "source": [
        "# Natural Language Processing (NLP)\n",
        "# First: Text Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-hHakcwxXdq"
      },
      "source": [
        "\n",
        "\n",
        "## üîç What is Natural Language Processing?\n",
        "\n",
        "**Natural Language Processing (NLP)** is a branch of Artificial Intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language in a meaningful and useful way.\n",
        "\n",
        "It combines:\n",
        "- **Computational Linguistics**\n",
        "- **Machine Learning**\n",
        "- **Deep Learning**\n",
        "\n",
        "to process and analyze large amounts of natural language data.\n",
        "\n",
        "### üéØ Key Objectives of NLP:\n",
        "- **Understanding**: Extracting meaning from language.\n",
        "- **Generation**: Producing human-like text.\n",
        "- **Translation**: Converting text between languages.\n",
        "- **Interaction**: Facilitating natural human-computer communication.\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Core NLP Terminology\n",
        "\n",
        "### üìö Fundamental Concepts\n",
        "\n",
        "| Term            | Description                                                                                  | Example                              |\n",
        "|-----------------|----------------------------------------------------------------------------------------------|--------------------------------------|\n",
        "| **Corpus**      | A large collection of texts used for training NLP models.                                     | Brown Corpus, Reuters Corpus         |\n",
        "| **Token**       | The smallest unit of text (word, character, or subword) obtained after tokenization.          | \"Hello world!\" ‚Üí `[\"Hello\", \"world\", \"!\"]` |\n",
        "| **Tokenization**| The process of breaking text into tokens.                                                     | Input: \"It's great!\" ‚Üí `[\"It\", \"'s\", \"great\", \"!\"]` |\n",
        "| **Vocabulary**  | The set of unique tokens in a corpus (also called lexicon).                                   | {\"Hello\", \"world\", \"!\"}              |\n",
        "\n",
        "### üí° Notes:\n",
        "- Tokenization can be complex due to punctuation, contractions, and language-specific rules.\n",
        "- The size of the vocabulary can impact both model performance and resource requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Getting Started with NLTK and spaCy\n",
        "\n",
        "Now that we understand the theoretical foundations of NLP, let‚Äôs dive into practical implementation using two of the most popular NLP libraries: **NLTK** and **spaCy**.\n",
        "\n",
        "---\n",
        "\n",
        "### üêç NLTK (Natural Language Toolkit)\n",
        "\n",
        "**NLTK** is a comprehensive Python library designed for working with human language data. It provides simple interfaces to over 50 corpora and lexical resources and is widely used in education, research, and rapid prototyping.\n",
        "\n",
        "#### ‚≠ê Key Features of NLTK:\n",
        "- üìö Extensive collection of text processing libraries\n",
        "- üóÇÔ∏è Built-in corpora and datasets for experimentation\n",
        "- üéì Educational focus with detailed tutorials and documentation\n",
        "- üîç Wide range of algorithms for:\n",
        "  - Tokenization\n",
        "  - Stemming\n",
        "  - Tagging\n",
        "  - Parsing\n",
        "  - Classification\n",
        "  - Semantic reasoning\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zqyq-FtEshBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1- **NLTK Installation**"
      ],
      "metadata": {
        "id": "Tkf2Z2tet6Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOrV-Flnt5qE",
        "outputId": "5dda55b3-033b-4f23-b0e0-d4f064c7c145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"\n",
        "Natural Language Processing is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables computers to understand,\n",
        "interpret, and generate human language in a meaningful way. NLP has revolutionized\n",
        "how we interact with technology, from search engines to chatbots.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Gk2_Skmvu4B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text_arabic = \"\"\"\n",
        "ÿ¨ÿßŸÖÿπÿ© ÿßŸÑŸÖŸÑŸÉ ÿπÿ®ÿØÿßŸÑŸÑŸá ŸÑŸÑÿπŸÑŸàŸÖ ŸàÿßŸÑÿ™ŸÇŸÜŸäÿ© ŸáŸä ÿ¨ÿßŸÖÿπÿ© ÿ®ÿ≠ÿ´Ÿäÿ© ŸÖÿ™ÿ∑Ÿàÿ±ÿ© ÿ™ŸÇÿπ ŸÅŸä ÿßŸÑŸÖŸÖŸÑŸÉÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑÿ≥ÿπŸàÿØŸäÿ©.\n",
        "ÿ™ÿ£ÿ≥ÿ≥ÿ™ ÿßŸÑÿ¨ÿßŸÖÿπÿ© ÿπÿßŸÖ 2009 Ÿàÿ™ŸáÿØŸÅ ÿ•ŸÑŸâ ÿ£ŸÜ ÿ™ŸÉŸàŸÜ ŸÖŸÜÿßÿ±ÿ© ŸÑŸÑÿπŸÑŸÖ ŸàÿßŸÑŸÖÿπÿ±ŸÅÿ© ŸÅŸä ÿßŸÑŸÖŸÜÿ∑ŸÇÿ© ŸàÿßŸÑÿπÿßŸÑŸÖ.\n",
        "ÿ™ÿ∂ŸÖ ÿßŸÑÿ¨ÿßŸÖÿπÿ© ÿ£ÿ≠ÿØÿ´ ÿßŸÑŸÖÿÆÿ™ÿ®ÿ±ÿßÿ™ ŸàÿßŸÑŸÖÿ±ÿßŸÅŸÇ ÿßŸÑÿ®ÿ≠ÿ´Ÿäÿ©ÿå Ÿàÿ™ÿ¨ÿ∞ÿ® ÿßŸÑÿ∑ŸÑÿßÿ® ŸàÿßŸÑÿ®ÿßÿ≠ÿ´ŸäŸÜ ŸÖŸÜ ÿ¨ŸÖŸäÿπ ÿ£ŸÜÿ≠ÿßÿ° ÿßŸÑÿπÿßŸÑŸÖ.\n",
        "ÿ™ÿ™ŸÖŸäÿ≤ ŸÉÿßŸàÿ≥ÿ™ ÿ®ÿ™ÿ±ŸÉŸäÿ≤Ÿáÿß ÿπŸÑŸâ ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿπŸÑŸÖŸä ŸàÿßŸÑÿßÿ®ÿ™ŸÉÿßÿ± ŸÅŸä ŸÖÿ¨ÿßŸÑÿßÿ™ ŸÖÿ™ÿπÿØÿØÿ© ŸÖÿ´ŸÑ ÿßŸÑŸáŸÜÿØÿ≥ÿ© ŸàÿßŸÑÿπŸÑŸàŸÖ ÿßŸÑÿ∑ÿ®ŸäÿπŸäÿ©\n",
        "ŸàÿßŸÑÿ≠ÿßÿ≥Ÿàÿ® ŸàÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä. ÿ™ÿ≥ÿπŸâ ÿßŸÑÿ¨ÿßŸÖÿπÿ© ŸÑÿ≠ŸÑ ÿßŸÑÿ™ÿ≠ÿØŸäÿßÿ™ ÿßŸÑÿπÿßŸÑŸÖŸäÿ© ŸÖŸÜ ÿÆŸÑÿßŸÑ ÿßŸÑÿ®ÿ≠ÿ´ ŸàÿßŸÑÿ™ÿ∑ŸàŸäÿ±.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "IMVgjS9qu263"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÇÔ∏è 2. **Tokenization Function**\n",
        "\n",
        "**Tokenization** is the process of splitting text into smaller meaningful units called **tokens**. These tokens are often words, subwords, or even characters, depending on the task.\n",
        "\n",
        "Tokenization is usually the **first step in text preprocessing** because most NLP models require structured input instead of raw text.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Why is Tokenization Important?\n",
        "\n",
        "- ‚úÖ Converts unstructured text into manageable pieces\n",
        "- ‚úÖ Enables word-level feature extraction and analysis\n",
        "- ‚úÖ Impacts vocabulary size and downstream model performance\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "UdvB-wIYzmV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDgd9RkhvsW9",
        "outputId": "350b6307-e7a0-4aae-833c-b7f33d055446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(sample_text)\n",
        "sentences = sent_tokenize(sample_text)\n",
        "print (words)\n",
        "print (sentences)\n",
        "print(f\"Number of words: {len(words)}\")\n",
        "print(f\"Number of sentences: {len(sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDd3hklexYdy",
        "outputId": "e2992221-ac20-4d68-e6cc-8ce3a892d0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'meaningful', 'way', '.', 'NLP', 'has', 'revolutionized', 'how', 'we', 'interact', 'with', 'technology', ',', 'from', 'search', 'engines', 'to', 'chatbots', '.']\n",
            "['\\nNatural Language Processing is a fascinating field that combines linguistics,\\ncomputer science, and artificial intelligence.', 'It enables computers to understand,\\ninterpret, and generate human language in a meaningful way.', 'NLP has revolutionized\\nhow we interact with technology, from search engines to chatbots.']\n",
            "Number of words: 50\n",
            "Number of sentences: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Arabic_words = word_tokenize(sample_text_arabic)\n",
        "Arabic_sentences = sent_tokenize(sample_text_arabic)\n",
        "print (Arabic_words)\n",
        "print (Arabic_sentences)\n",
        "print(f\"Number of words: {len(Arabic_words)}\")\n",
        "print(f\"Number of sentences: {len(Arabic_sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E50xGh_Hy8CR",
        "outputId": "fa32cc77-9c43-47f9-a4bc-6ae3e61c21f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ÿ¨ÿßŸÖÿπÿ©', 'ÿßŸÑŸÖŸÑŸÉ', 'ÿπÿ®ÿØÿßŸÑŸÑŸá', 'ŸÑŸÑÿπŸÑŸàŸÖ', 'ŸàÿßŸÑÿ™ŸÇŸÜŸäÿ©', 'ŸáŸä', 'ÿ¨ÿßŸÖÿπÿ©', 'ÿ®ÿ≠ÿ´Ÿäÿ©', 'ŸÖÿ™ÿ∑Ÿàÿ±ÿ©', 'ÿ™ŸÇÿπ', 'ŸÅŸä', 'ÿßŸÑŸÖŸÖŸÑŸÉÿ©', 'ÿßŸÑÿπÿ±ÿ®Ÿäÿ©', 'ÿßŸÑÿ≥ÿπŸàÿØŸäÿ©', '.', 'ÿ™ÿ£ÿ≥ÿ≥ÿ™', 'ÿßŸÑÿ¨ÿßŸÖÿπÿ©', 'ÿπÿßŸÖ', '2009', 'Ÿàÿ™ŸáÿØŸÅ', 'ÿ•ŸÑŸâ', 'ÿ£ŸÜ', 'ÿ™ŸÉŸàŸÜ', 'ŸÖŸÜÿßÿ±ÿ©', 'ŸÑŸÑÿπŸÑŸÖ', 'ŸàÿßŸÑŸÖÿπÿ±ŸÅÿ©', 'ŸÅŸä', 'ÿßŸÑŸÖŸÜÿ∑ŸÇÿ©', 'ŸàÿßŸÑÿπÿßŸÑŸÖ', '.', 'ÿ™ÿ∂ŸÖ', 'ÿßŸÑÿ¨ÿßŸÖÿπÿ©', 'ÿ£ÿ≠ÿØÿ´', 'ÿßŸÑŸÖÿÆÿ™ÿ®ÿ±ÿßÿ™', 'ŸàÿßŸÑŸÖÿ±ÿßŸÅŸÇ', 'ÿßŸÑÿ®ÿ≠ÿ´Ÿäÿ©ÿå', 'Ÿàÿ™ÿ¨ÿ∞ÿ®', 'ÿßŸÑÿ∑ŸÑÿßÿ®', 'ŸàÿßŸÑÿ®ÿßÿ≠ÿ´ŸäŸÜ', 'ŸÖŸÜ', 'ÿ¨ŸÖŸäÿπ', 'ÿ£ŸÜÿ≠ÿßÿ°', 'ÿßŸÑÿπÿßŸÑŸÖ', '.', 'ÿ™ÿ™ŸÖŸäÿ≤', 'ŸÉÿßŸàÿ≥ÿ™', 'ÿ®ÿ™ÿ±ŸÉŸäÿ≤Ÿáÿß', 'ÿπŸÑŸâ', 'ÿßŸÑÿ®ÿ≠ÿ´', 'ÿßŸÑÿπŸÑŸÖŸä', 'ŸàÿßŸÑÿßÿ®ÿ™ŸÉÿßÿ±', 'ŸÅŸä', 'ŸÖÿ¨ÿßŸÑÿßÿ™', 'ŸÖÿ™ÿπÿØÿØÿ©', 'ŸÖÿ´ŸÑ', 'ÿßŸÑŸáŸÜÿØÿ≥ÿ©', 'ŸàÿßŸÑÿπŸÑŸàŸÖ', 'ÿßŸÑÿ∑ÿ®ŸäÿπŸäÿ©', 'ŸàÿßŸÑÿ≠ÿßÿ≥Ÿàÿ®', 'ŸàÿßŸÑÿ∞ŸÉÿßÿ°', 'ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä', '.', 'ÿ™ÿ≥ÿπŸâ', 'ÿßŸÑÿ¨ÿßŸÖÿπÿ©', 'ŸÑÿ≠ŸÑ', 'ÿßŸÑÿ™ÿ≠ÿØŸäÿßÿ™', 'ÿßŸÑÿπÿßŸÑŸÖŸäÿ©', 'ŸÖŸÜ', 'ÿÆŸÑÿßŸÑ', 'ÿßŸÑÿ®ÿ≠ÿ´', 'ŸàÿßŸÑÿ™ÿ∑ŸàŸäÿ±', '.']\n",
            "['\\nÿ¨ÿßŸÖÿπÿ© ÿßŸÑŸÖŸÑŸÉ ÿπÿ®ÿØÿßŸÑŸÑŸá ŸÑŸÑÿπŸÑŸàŸÖ ŸàÿßŸÑÿ™ŸÇŸÜŸäÿ© ŸáŸä ÿ¨ÿßŸÖÿπÿ© ÿ®ÿ≠ÿ´Ÿäÿ© ŸÖÿ™ÿ∑Ÿàÿ±ÿ© ÿ™ŸÇÿπ ŸÅŸä ÿßŸÑŸÖŸÖŸÑŸÉÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑÿ≥ÿπŸàÿØŸäÿ©.', 'ÿ™ÿ£ÿ≥ÿ≥ÿ™ ÿßŸÑÿ¨ÿßŸÖÿπÿ© ÿπÿßŸÖ 2009 Ÿàÿ™ŸáÿØŸÅ ÿ•ŸÑŸâ ÿ£ŸÜ ÿ™ŸÉŸàŸÜ ŸÖŸÜÿßÿ±ÿ© ŸÑŸÑÿπŸÑŸÖ ŸàÿßŸÑŸÖÿπÿ±ŸÅÿ© ŸÅŸä ÿßŸÑŸÖŸÜÿ∑ŸÇÿ© ŸàÿßŸÑÿπÿßŸÑŸÖ.', 'ÿ™ÿ∂ŸÖ ÿßŸÑÿ¨ÿßŸÖÿπÿ© ÿ£ÿ≠ÿØÿ´ ÿßŸÑŸÖÿÆÿ™ÿ®ÿ±ÿßÿ™ ŸàÿßŸÑŸÖÿ±ÿßŸÅŸÇ ÿßŸÑÿ®ÿ≠ÿ´Ÿäÿ©ÿå Ÿàÿ™ÿ¨ÿ∞ÿ® ÿßŸÑÿ∑ŸÑÿßÿ® ŸàÿßŸÑÿ®ÿßÿ≠ÿ´ŸäŸÜ ŸÖŸÜ ÿ¨ŸÖŸäÿπ ÿ£ŸÜÿ≠ÿßÿ° ÿßŸÑÿπÿßŸÑŸÖ.', 'ÿ™ÿ™ŸÖŸäÿ≤ ŸÉÿßŸàÿ≥ÿ™ ÿ®ÿ™ÿ±ŸÉŸäÿ≤Ÿáÿß ÿπŸÑŸâ ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿπŸÑŸÖŸä ŸàÿßŸÑÿßÿ®ÿ™ŸÉÿßÿ± ŸÅŸä ŸÖÿ¨ÿßŸÑÿßÿ™ ŸÖÿ™ÿπÿØÿØÿ© ŸÖÿ´ŸÑ ÿßŸÑŸáŸÜÿØÿ≥ÿ© ŸàÿßŸÑÿπŸÑŸàŸÖ ÿßŸÑÿ∑ÿ®ŸäÿπŸäÿ©\\nŸàÿßŸÑÿ≠ÿßÿ≥Ÿàÿ® ŸàÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä.', 'ÿ™ÿ≥ÿπŸâ ÿßŸÑÿ¨ÿßŸÖÿπÿ© ŸÑÿ≠ŸÑ ÿßŸÑÿ™ÿ≠ÿØŸäÿßÿ™ ÿßŸÑÿπÿßŸÑŸÖŸäÿ© ŸÖŸÜ ÿÆŸÑÿßŸÑ ÿßŸÑÿ®ÿ≠ÿ´ ŸàÿßŸÑÿ™ÿ∑ŸàŸäÿ±.']\n",
            "Number of words: 72\n",
            "Number of sentences: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üö´ 3. Stop Words Removal Function\n",
        "\n",
        "**Stop words** are commonly used words in a language that carry little meaningful information for NLP tasks. Examples include:  \n",
        "**\"the\"**, **\"is\"**, **\"in\"**, **\"and\"**, **\"on\"**, etc.\n",
        "\n",
        "Removing stop words is a common preprocessing step that helps:\n",
        "- üßπ Reduce noise in the data\n",
        "- üìâ Decrease vocabulary size\n",
        "- üöÄ Improve model efficiency and focus on important words\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xWM9cD7Tz_Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]\n",
        "print(f\"Original words: {len(words)}\")\n",
        "print(f\"After removing stop words: {len(filtered_words)}\")\n",
        "print(f\"Filtered words: {filtered_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByiL9N940mUo",
        "outputId": "343e002b-28cf-44b7-9e87-09c9ba846e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: 50\n",
            "After removing stop words: 27\n",
            "Filtered words: ['natural', 'language', 'processing', 'fascinating', 'field', 'combines', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'enables', 'computers', 'understand', 'interpret', 'generate', 'human', 'language', 'meaningful', 'way', 'nlp', 'revolutionized', 'interact', 'technology', 'search', 'engines', 'chatbots']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('arabic'))\n",
        "Arabic_filtered_words = [word.lower() for word in Arabic_words if word.lower() not in stop_words and word.isalpha()]\n",
        "\n",
        "print(f\"Original words: {len(Arabic_words)}\")\n",
        "print(f\"After removing stop words: {len(Arabic_filtered_words)}\")\n",
        "print(f\"Filtered words: {Arabic_filtered_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gXwU86n0DfV",
        "outputId": "ed17fbaf-d6dd-4da8-d22a-2c7a03996f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: 72\n",
            "After removing stop words: 54\n",
            "Filtered words: ['ÿ¨ÿßŸÖÿπÿ©', 'ÿßŸÑŸÖŸÑŸÉ', 'ÿπÿ®ÿØÿßŸÑŸÑŸá', 'ŸÑŸÑÿπŸÑŸàŸÖ', 'ŸàÿßŸÑÿ™ŸÇŸÜŸäÿ©', 'ÿ¨ÿßŸÖÿπÿ©', 'ÿ®ÿ≠ÿ´Ÿäÿ©', 'ŸÖÿ™ÿ∑Ÿàÿ±ÿ©', 'ÿ™ŸÇÿπ', 'ÿßŸÑŸÖŸÖŸÑŸÉÿ©', 'ÿßŸÑÿπÿ±ÿ®Ÿäÿ©', 'ÿßŸÑÿ≥ÿπŸàÿØŸäÿ©', 'ÿ™ÿ£ÿ≥ÿ≥ÿ™', 'ÿßŸÑÿ¨ÿßŸÖÿπÿ©', 'ÿπÿßŸÖ', 'Ÿàÿ™ŸáÿØŸÅ', 'ÿ™ŸÉŸàŸÜ', 'ŸÖŸÜÿßÿ±ÿ©', 'ŸÑŸÑÿπŸÑŸÖ', 'ŸàÿßŸÑŸÖÿπÿ±ŸÅÿ©', 'ÿßŸÑŸÖŸÜÿ∑ŸÇÿ©', 'ŸàÿßŸÑÿπÿßŸÑŸÖ', 'ÿ™ÿ∂ŸÖ', 'ÿßŸÑÿ¨ÿßŸÖÿπÿ©', 'ÿ£ÿ≠ÿØÿ´', 'ÿßŸÑŸÖÿÆÿ™ÿ®ÿ±ÿßÿ™', 'ŸàÿßŸÑŸÖÿ±ÿßŸÅŸÇ', 'Ÿàÿ™ÿ¨ÿ∞ÿ®', 'ÿßŸÑÿ∑ŸÑÿßÿ®', 'ŸàÿßŸÑÿ®ÿßÿ≠ÿ´ŸäŸÜ', 'ÿ£ŸÜÿ≠ÿßÿ°', 'ÿßŸÑÿπÿßŸÑŸÖ', 'ÿ™ÿ™ŸÖŸäÿ≤', 'ŸÉÿßŸàÿ≥ÿ™', 'ÿ®ÿ™ÿ±ŸÉŸäÿ≤Ÿáÿß', 'ÿßŸÑÿ®ÿ≠ÿ´', 'ÿßŸÑÿπŸÑŸÖŸä', 'ŸàÿßŸÑÿßÿ®ÿ™ŸÉÿßÿ±', 'ŸÖÿ¨ÿßŸÑÿßÿ™', 'ŸÖÿ™ÿπÿØÿØÿ©', 'ÿßŸÑŸáŸÜÿØÿ≥ÿ©', 'ŸàÿßŸÑÿπŸÑŸàŸÖ', 'ÿßŸÑÿ∑ÿ®ŸäÿπŸäÿ©', 'ŸàÿßŸÑÿ≠ÿßÿ≥Ÿàÿ®', 'ŸàÿßŸÑÿ∞ŸÉÿßÿ°', 'ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä', 'ÿ™ÿ≥ÿπŸâ', 'ÿßŸÑÿ¨ÿßŸÖÿπÿ©', 'ŸÑÿ≠ŸÑ', 'ÿßŸÑÿ™ÿ≠ÿØŸäÿßÿ™', 'ÿßŸÑÿπÿßŸÑŸÖŸäÿ©', 'ÿÆŸÑÿßŸÑ', 'ÿßŸÑÿ®ÿ≠ÿ´', 'ŸàÿßŸÑÿ™ÿ∑ŸàŸäÿ±']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå± 4. **Stemming Function**\n",
        "\n",
        "**Stemming** is the process of reducing words to their **base or root form** by removing suffixes and prefixes. The resulting stem may not always be a valid word but represents a group of related words.\n",
        "\n",
        "Example:  \n",
        "`\"running\"`, `\"runner\"`, `\"runs\"` ‚Üí `\"run\"`\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Why is Stemming Useful?\n",
        "\n",
        "- ‚úÖ Reduces different forms of a word to a common base\n",
        "- ‚úÖ Helps in grouping words with similar meaning\n",
        "- ‚úÖ Decreases vocabulary size and improves generalization\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gNaSaC1704um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHaU1vor14cN",
        "outputId": "b3b069d8-1442-429f-cfe3-b24a1de6ffa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natur', 'languag', 'process', 'is', 'a', 'fascin', 'field', 'that', 'combin', 'linguist', ',', 'comput', 'scienc', ',', 'and', 'artifici', 'intellig', '.', 'it', 'enabl', 'comput', 'to', 'understand', ',', 'interpret', ',', 'and', 'gener', 'human', 'languag', 'in', 'a', 'meaning', 'way', '.', 'nlp', 'ha', 'revolution', 'how', 'we', 'interact', 'with', 'technolog', ',', 'from', 'search', 'engin', 'to', 'chatbot', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.isri import ISRIStemmer\n",
        "\n",
        "# Arabic stemmer\n",
        "stemmer = ISRIStemmer()\n",
        "Arabic_stemmed_words = [stemmer.stem(word) for word in Arabic_filtered_words]\n",
        "print(f\"Original words: {Arabic_filtered_words[:10]}\")\n",
        "print(f\"Stemmed words: {Arabic_stemmed_words[:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIOQPUyX33Rm",
        "outputId": "7261ea03-69a2-4963-fa6e-9fd38e303462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['ÿ¨ÿßŸÖÿπÿ©', 'ÿßŸÑŸÖŸÑŸÉ', 'ÿπÿ®ÿØÿßŸÑŸÑŸá', 'ŸÑŸÑÿπŸÑŸàŸÖ', 'ŸàÿßŸÑÿ™ŸÇŸÜŸäÿ©', 'ÿ¨ÿßŸÖÿπÿ©', 'ÿ®ÿ≠ÿ´Ÿäÿ©', 'ŸÖÿ™ÿ∑Ÿàÿ±ÿ©', 'ÿ™ŸÇÿπ', 'ÿßŸÑŸÖŸÖŸÑŸÉÿ©']\n",
            "Stemmed words: ['ÿ¨ŸÖÿπ', 'ŸÖŸÑŸÉ', 'ÿπÿ®ÿØÿßŸÑŸÑ', 'ÿπŸÑŸÖ', 'ŸÇŸÜŸä', 'ÿ¨ŸÖÿπ', 'ÿ®ÿ≠ÿ´', 'ÿ™ÿ∑ÿ±', 'ÿ™ŸÇÿπ', 'ŸÖŸÑŸÉ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üçÉ 5-**Lemmatization Function**\n",
        "\n",
        "**Lemmatization** is the process of reducing words to their **dictionary base form** (called the *lemma*) while ensuring the result is a **valid word**. Unlike stemming, lemmatization uses **context and vocabulary** to produce meaningful roots.\n",
        "\n",
        "Example:  \n",
        "`\"running\"` ‚Üí `\"run\"`  \n",
        "`\"better\"` ‚Üí `\"good\"`\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Why Use Lemmatization?\n",
        "\n",
        "- ‚úÖ Produces linguistically correct root words\n",
        "- ‚úÖ Considers **part of speech (POS)** for accurate results\n",
        "- ‚úÖ More precise than stemming, especially in formal NLP tasks\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9iCI6-Es4eBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqwWrWZC522G",
        "outputId": "92d6cd5a-b1b9-46bb-e32e-29bc56682214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'that', 'combine', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'computer', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'meaningful', 'way', '.', 'NLP', 'ha', 'revolutionized', 'how', 'we', 'interact', 'with', 'technology', ',', 'from', 'search', 'engine', 'to', 'chatbots', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6-**Part-of-Speech Tagging**\n",
        "\n",
        "**Part-of-Speech (POS) Tagging** is the process of labeling each word in a sentence with its **grammatical role** such as noun, verb, adjective, etc.\n",
        "\n",
        "POS tagging is essential in NLP for understanding sentence structure, syntactic parsing, named entity recognition, and more.\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Part-of-Speech (POS) Tags Reference\n",
        "\n",
        "| Tag   | Meaning                             | Example Words            |\n",
        "|-------|-------------------------------------|--------------------------|\n",
        "| **CC**   | Coordinating conjunction            | and, but, or             |\n",
        "| **CD**   | Cardinal number                    | one, two, 1, 2           |\n",
        "| **DT**   | Determiner                         | the, a, an               |\n",
        "| **EX**   | Existential 'there'                | there (as in *there is*) |\n",
        "| **FW**   | Foreign word                       | bonjour, sayonara        |\n",
        "| **IN**   | Preposition or subordinating conj. | in, of, like, after      |\n",
        "| **JJ**   | Adjective                          | big, blue, smart         |\n",
        "| **JJR**  | Adjective, comparative             | bigger, smarter          |\n",
        "| **JJS**  | Adjective, superlative             | biggest, smartest        |\n",
        "| **LS**   | List item marker                   | 1., A, B                 |\n",
        "| **MD**   | Modal verb                         | can, could, will, would  |\n",
        "| **NN**   | Noun, singular                     | cat, house, idea         |\n",
        "| **NNS**  | Noun, plural                       | cats, houses, ideas      |\n",
        "| **NNP**  | Proper noun, singular              | John, London, Apple      |\n",
        "| **NNPS** | Proper noun, plural                | Americans, Smiths        |\n",
        "| **PDT**  | Predeterminer                      | all, both, half          |\n",
        "| **POS**  | Possessive ending                  | 's (as in *John's*)      |\n",
        "| **PRP**  | Personal pronoun                   | I, he, she, it           |\n",
        "| **PRP$** | Possessive pronoun                 | my, his, her, its        |\n",
        "| **RB**   | Adverb                             | quickly, very, well      |\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Quick Reference for Common POS Categories\n",
        "\n",
        "| Category        | POS Tags                          | Examples              |\n",
        "|-----------------|-------------------------------------|-----------------------|\n",
        "| **Nouns**       | NN, NNS, NNP, NNPS                 | cat, London, ideas    |\n",
        "| **Verbs**       | VB, VBD, VBG, VBN, VBP, VBZ        | run, ran, running     |\n",
        "| **Adjectives**  | JJ, JJR, JJS                       | blue, smarter, biggest |\n",
        "| **Adverbs**     | RB, RBR, RBS                       | quickly, more, most   |\n",
        "| **Pronouns**    | PRP, PRP$, WP, WP$                 | he, she, whose        |\n",
        "| **Determiners** | DT, WDT                            | the, which            |\n",
        "| **Prepositions**| IN                                  | in, on, after         |\n",
        "| **Conjunctions**| CC                                  | and, but, or          |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6mFCOsw8G-yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "pos_tags = pos_tag(words)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_AQ4RMEE3hW",
        "outputId": "f5bd326f-01b2-4e5e-9bfa-b5db8c35488b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('fascinating', 'JJ'), ('field', 'NN'), ('that', 'WDT'), ('combines', 'VBZ'), ('linguistics', 'NNS'), (',', ','), ('computer', 'NN'), ('science', 'NN'), (',', ','), ('and', 'CC'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('.', '.'), ('It', 'PRP'), ('enables', 'VBZ'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), (',', ','), ('interpret', 'VB'), (',', ','), ('and', 'CC'), ('generate', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('in', 'IN'), ('a', 'DT'), ('meaningful', 'JJ'), ('way', 'NN'), ('.', '.'), ('NLP', 'NNP'), ('has', 'VBZ'), ('revolutionized', 'VBN'), ('how', 'WRB'), ('we', 'PRP'), ('interact', 'VBP'), ('with', 'IN'), ('technology', 'NN'), (',', ','), ('from', 'IN'), ('search', 'NN'), ('engines', 'NNS'), ('to', 'TO'), ('chatbots', 'NNS'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# üòä 7- Sentiment Analysis\n",
        "\n",
        "**Sentiment Analysis** is the process of identifying and categorizing emotions or opinions expressed in a piece of text‚Äîtypically as **positive**, **negative**, or **neutral**.\n",
        "\n",
        "In **NLTK**, we can perform sentiment analysis using the built-in **VADER** sentiment analyzer, which is particularly effective for short texts like social media posts.\n",
        "## ‚ùì Why is it Called **VADER**?\n",
        "\n",
        "**VADER** stands for:\n",
        "\n",
        "> **V**alence **A**ware **D**ictionary and s**E**ntiment **R**easoner\n",
        "\n",
        "---\n",
        "\n",
        "### üí° What Does That Mean?\n",
        "\n",
        "- **Valence**: The **emotional value** of a word or phrase (positive, negative, or neutral).\n",
        "- **Aware**: It considers not just the words, but also how they are used in context.\n",
        "- **Dictionary**: Uses a **predefined lexicon** of words with associated sentiment scores.\n",
        "- **Sentiment Reasoner**: Applies **rules** to adjust sentiment based on:\n",
        "  - Punctuation (e.g., ‚Äú!!!‚Äù adds intensity)\n",
        "  - Capitalization (e.g., ‚ÄúLOVE‚Äù is stronger than ‚Äúlove‚Äù)\n",
        "  - Degree modifiers (e.g., ‚Äúvery good‚Äù vs. ‚Äúgood‚Äù)\n",
        "  - Emojis and emoticons (e.g., üòä üò¢)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "q9z7RloYHj8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "def nltk_sentiment_analysis(text):\n",
        "    \"\"\"Analyze sentiment of text\"\"\"\n",
        "    print(\"=== SENTIMENT ANALYSIS ===\")\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "    print(f\"Sentiment scores: {sentiment_scores}\")\n",
        "\n",
        "    # Interpret the scores\n",
        "    if sentiment_scores['compound'] >= 0.05:\n",
        "        print(\"Overall sentiment: Positive\")\n",
        "    elif sentiment_scores['compound'] <= -0.05:\n",
        "        print(\"Overall sentiment: Negative\")\n",
        "    else:\n",
        "        print(\"Overall sentiment: Neutral\")\n",
        "\n",
        "    return sentiment_scores\n",
        "\n",
        "# Run the function\n",
        "sentiment_scores = nltk_sentiment_analysis(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvUp6oO6HjOY",
        "outputId": "e41e472a-5098-46c8-8964-3f331fccdf2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SENTIMENT ANALYSIS ===\n",
            "Sentiment scores: {'neg': 0.0, 'neu': 0.759, 'pos': 0.241, 'compound': 0.886}\n",
            "Overall sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ **8-spaCy**\n",
        "\n",
        "**spaCy** is an **industrial-strength Natural Language Processing (NLP) library** built for **production-level applications**. It is known for being **fast, efficient, and accurate**, with easy integration into real-world systems.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚≠ê Key Features of spaCy:\n",
        "\n",
        "- ‚ö° **High Performance:** Optimized for speed and large-scale processing.\n",
        "- üåê **Pre-trained Models:** Supports multiple languages (English, Arabic, German, etc.).\n",
        "- üß† **Deep Learning Ready:** Seamless integration with deep learning frameworks (e.g., PyTorch, TensorFlow).\n",
        "- üèóÔ∏è **Production Focused:** Designed for real-world use, not just research or prototyping.\n",
        "- üîó **Advanced NLP Tasks:**\n",
        "  - **Tokenization**\n",
        "  - **Part-of-Speech Tagging (POS)**\n",
        "  - **Dependency Parsing**\n",
        "  - **Named Entity Recognition (NER)**\n",
        "  - **Text Classification**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VdCU1x_oIc5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9-**spaCy Installation**"
      ],
      "metadata": {
        "id": "gc2CeNc9IgCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpRoHOLxInVD",
        "outputId": "686df5bf-54d0-49e4-a6dc-21d70de6b51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Process the text\n",
        "doc = nlp(sample_text)\n"
      ],
      "metadata": {
        "id": "rLd3l565I-LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üè∑Ô∏è 10. Named Entity Recognition (NER)\n",
        "\n",
        "## üìå What is Named Entity Recognition (NER)?\n",
        "\n",
        "**Named Entity Recognition (NER)** is a key task in Natural Language Processing (NLP) where the goal is to **automatically detect and classify named entities** in text into predefined categories such as:\n",
        "- **Persons**\n",
        "- **Organizations**\n",
        "- **Locations**\n",
        "- **Dates**\n",
        "- **Monetary values**\n",
        "- And more...\n",
        "\n",
        "NER is useful in a variety of applications including:\n",
        "- üìä **Information extraction**\n",
        "- üîé **Search engines**\n",
        "- üì∞ **News analysis**\n",
        "- üìà **Business intelligence**\n",
        "- üí¨ **Chatbots and virtual assistants**\n",
        "\n",
        "---\n",
        "\n",
        "## üìù How Does NER Work in spaCy?\n",
        "\n",
        "- spaCy's pre-trained models can **identify named entities** directly from raw text.\n",
        "- Entities are returned with:\n",
        "  - **The text span** (e.g., `\"Apple\"`)\n",
        "  - **The entity type** (e.g., `ORG` for Organization)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JKTFYevbKLSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model once\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def detailed_ner_analysis(text):\n",
        "    \"\"\"\n",
        "    Perform Named Entity Recognition (NER) on the given text using spaCy.\n",
        "    Displays entities with their labels and explanations, along with token details.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "\n",
        "    print(\"\\n=== NAMED ENTITY RECOGNITION (NER) ===\")\n",
        "    print(f\"üìÑ Text length: {len(text)} characters\")\n",
        "    print(f\"üî¢ Number of tokens: {len(doc)}\")\n",
        "    print(f\"üîç Number of entities: {len(doc.ents)}\\n\")\n",
        "\n",
        "    if doc.ents:\n",
        "        print(\"üóÇ Entities Found:\")\n",
        "        for ent in doc.ents:\n",
        "            label_desc = spacy.explain(ent.label_) or \"No description available\"\n",
        "            print(f\"- {ent.text}  ‚ûî  {ent.label_} ({label_desc})\")\n",
        "    else:\n",
        "        print(\"No entities detected in the text.\")\n",
        "\n",
        "    print(\"\\nüî† First 10 Tokens (with POS and Entity Type):\")\n",
        "    for token in doc[:10]:\n",
        "        ent_type = token.ent_type_ if token.ent_type_ else \"None\"\n",
        "        print(f\"{token.text:<15} | {token.pos_:<10} | {ent_type}\")\n",
        "\n",
        "    return doc.ents\n",
        "\n",
        "\n",
        "\n",
        "# Run the analysis\n",
        "entities = detailed_ner_analysis(sample_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xe5v82JJdaF",
        "outputId": "f4ccccd2-e251-421b-951d-1f2b442beae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== NAMED ENTITY RECOGNITION (NER) ===\n",
            "üìÑ Text length: 311 characters\n",
            "üî¢ Number of tokens: 55\n",
            "üîç Number of entities: 1\n",
            "\n",
            "üóÇ Entities Found:\n",
            "- NLP  ‚ûî  ORG (Companies, agencies, institutions, etc.)\n",
            "\n",
            "üî† First 10 Tokens (with POS and Entity Type):\n",
            "\n",
            "               | SPACE      | None\n",
            "Natural         | PROPN      | None\n",
            "Language        | PROPN      | None\n",
            "Processing      | NOUN       | None\n",
            "is              | AUX        | None\n",
            "a               | DET        | None\n",
            "fascinating     | ADJ        | None\n",
            "field           | NOUN       | None\n",
            "that            | PRON       | None\n",
            "combines        | VERB       | None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîó 11-Dependency Parsing - Common Dependency Relations\n",
        "\n",
        "## üìå What is Dependency Parsing?\n",
        "\n",
        "**Dependency Parsing** is the process of analyzing the **grammatical structure** of a sentence by identifying relationships between words. Each word (token) is connected to another word called its **head**, forming a tree-like structure.\n",
        "\n",
        "‚úÖ Every word has:\n",
        "- A **head word** it depends on (except the root word)\n",
        "- A **dependency relation** that explains the grammatical link\n",
        "\n",
        "---\n",
        "\n",
        "## üóÇÔ∏è Common Dependency Relations\n",
        "\n",
        "| Dependency | Full Name                    | Description                                  | Example (Relation)                   |\n",
        "|-----------|------------------------------|----------------------------------------------|--------------------------------------|\n",
        "| **ROOT**   | Root                          | Main verb or predicate of the sentence        | \"runs\" in \"John runs fast\"           |\n",
        "| **nsubj**  | Nominal subject               | Subject of the verb                          | \"John\" ‚Üí \"runs\"                      |\n",
        "| **nsubjpass** | Passive nominal subject     | Subject of passive verb                      | \"cake\" in \"The cake was eaten\"       |\n",
        "| **dobj**   | Direct object                 | Direct object of the verb                    | \"ball\" in \"John threw the ball\"      |\n",
        "| **iobj**   | Indirect object               | Indirect object of the verb                  | \"him\" in \"I gave him the book\"       |\n",
        "| **amod**   | Adjectival modifier          | Adjective modifying a noun                   | \"red\" ‚Üí \"car\"                        |\n",
        "| **advmod** | Adverbial modifier           | Adverb modifying verb/adj/adv                | \"quickly\" ‚Üí \"runs\"                   |\n",
        "| **det**    | Determiner                   | Article or determiner                        | \"the\" ‚Üí \"book\"                       |\n",
        "| **prep**   | Prepositional modifier       | Preposition linking to object                | \"in\" in \"in the house\"               |\n",
        "| **pobj**   | Object of preposition        | Object of the preposition                    | \"house\" in \"in the house\"            |\n",
        "| **aux**    | Auxiliary                    | Helping verb                                 | \"is\" in \"is running\"                 |\n",
        "| **auxpass** | Passive auxiliary            | Passive helping verb                         | \"was\" in \"was eaten\"                 |\n",
        "| **cop**    | Copula                       | Linking verb (be, seem, etc.)                | \"is\" in \"John is tall\"               |\n",
        "| **cc**     | Coordinating conjunction     | And, or, but                                 | \"and\" in \"cats and dogs\"             |\n",
        "| **conj**   | Conjunct                     | Coordinated element                          | \"dogs\" in \"cats and dogs\"            |\n",
        "| **compound** | Compound                    | Modifier in compound words                   | \"ice\" ‚Üí \"cream\"                      |\n",
        "| **poss**   | Possessive modifier          | Possessive noun or pronoun                   | \"John's\" ‚Üí \"book\"                    |\n",
        "| **appos**  | Appositional modifier        | Noun phrase renaming another noun            | \"CEO\" in \"John, the CEO\"             |\n",
        "| **acl**    | Adjectival clause            | Clause modifying a noun                      | \"who runs\" in \"man who runs\"         |\n",
        "| **advcl**  | Adverbial clause             | Clause modifying verb or adjective           | \"when he arrived\"                   |\n",
        "| **ccomp**  | Clausal complement           | Clause functioning as object                 | \"that he left\" in \"I think that...\"  |\n",
        "| **xcomp**  | Open clausal complement      | Non-finite clause complement                 | \"to run\" in \"I want to run\"          |\n",
        "| **mark**   | Marker                       | Subordinating conjunction                    | \"that\" in \"I know that...\"           |\n",
        "| **punct**  | Punctuation                  | Punctuation marks                            | \".\", \"!\", \"?\"                        |\n",
        "| **neg**    | Negation modifier            | Negation word                                | \"not\" in \"do not run\"                |\n",
        "| **prt**    | Particle                     | Verb particle                                | \"up\" in \"give up\"                    |\n",
        "| **quantmod** | Quantifier modifier         | Quantifier                                    | \"very\" in \"very good\"                |\n",
        "| **npadvmod** | Noun phrase adverbial modifier | Noun used adverbially                        | \"today\" in \"I'll go today\"           |\n",
        "| **tmod**   | Temporal modifier            | Time expression                              | \"yesterday\"                          |\n",
        "| **nummod** | Numeric modifier             | Numeric expression                           | \"five\" in \"five books\"               |\n",
        "| **number** | Number compound              | Part of complex number                       | \"twenty\" in \"twenty-five\"            |\n",
        "| **parataxis** | Parataxis                  | Parallel clauses                             | Quoted or separate sentences         |\n",
        "| **dep**    | Unspecified dependency       | Fallback relation when parser is unsure      | Miscellaneous links                  |\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Example Dependency Analysis\n",
        "\n",
        "**Sentence**:  \n",
        "*‚ÄúThe quick brown fox jumps over the lazy dog.‚Äù*\n",
        "\n",
        "| Token  | Dependency | Head   | Explanation                          |\n",
        "|--------|------------|--------|--------------------------------------|\n",
        "| The    | det        | fox    | Determiner modifies \"fox\"            |\n",
        "| quick  | amod       | fox    | Adjective modifies \"fox\"             |\n",
        "| brown  | amod       | fox    | Adjective modifies \"fox\"             |\n",
        "| fox    | nsubj      | jumps  | Subject of the verb                  |\n",
        "| jumps  | ROOT       | jumps  | Main verb                            |\n",
        "| over   | prep       | jumps  | Preposition attached to \"jumps\"      |\n",
        "| the    | det        | dog    | Determiner modifies \"dog\"            |\n",
        "| lazy   | amod       | dog    | Adjective modifies \"dog\"             |\n",
        "| dog    | pobj       | over   | Object of the preposition \"over\"     |\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Key Concepts in Dependency Parsing\n",
        "\n",
        "### üîó Head vs. Dependent:\n",
        "- **Head**: The main word that governs the relation (e.g., verb or noun).\n",
        "- **Dependent**: The word that is attached to and modifies the head.\n",
        "\n",
        "### üîÑ Common Grammar Patterns:\n",
        "| Pattern           | Example                             |\n",
        "|-------------------|-------------------------------------|\n",
        "| **Subject-Verb**  | `nsubj ‚Üí ROOT`                      |\n",
        "| **Verb-Object**   | `ROOT ‚Üí dobj`                       |\n",
        "| **Adj-Noun**      | `amod ‚Üí noun`                       |\n",
        "| **Det-Noun**      | `det ‚Üí noun`                        |\n",
        "| **Prep-Object**   | `prep ‚Üí pobj`                       |\n",
        "\n",
        "### üå≥ ROOT Token:\n",
        "- Every sentence has **one ROOT**.\n",
        "- It is usually the **main verb**.\n",
        "- All other tokens connect back to this ROOT.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Applications of Dependency Parsing:\n",
        "- üîç **Information Extraction:** Find key actors (subjects) and actions (verbs).\n",
        "- ‚ùì **Question Answering:** Identify important relationships for precise answers.\n",
        "- ‚úÇÔ∏è **Text Summarization:** Understand structure for better summarization.\n",
        "- üåê **Machine Translation:** Preserve grammatical correctness across languages.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2V9R7qRALr_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model and process text\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_dependency_parsing(text):\n",
        "    \"\"\"Show dependency relationships using spaCy\"\"\"\n",
        "    # Process the text through spaCy pipeline\n",
        "    doc = nlp(text)\n",
        "\n",
        "    print(\"=== DEPENDENCY PARSING ===\")\n",
        "    print(\"Token -> Dependency -> Head:\")\n",
        "\n",
        "    count = 0\n",
        "    for token in doc:\n",
        "        if token.is_alpha:  # Only show alphabetic tokens\n",
        "            print(f\"{token.text:15} | {token.dep_:10} | {token.head.text}\")\n",
        "            count += 1\n",
        "            if count >= 10:  # Limit to first 10 tokens\n",
        "                break\n",
        "\n",
        "    return doc\n",
        "\n",
        "# Run the function\n",
        "doc = spacy_dependency_parsing(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvPDTNhiLV23",
        "outputId": "a1ddf916-0e5b-4806-ccac-4d15d0b4a046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DEPENDENCY PARSING ===\n",
            "Token -> Dependency -> Head:\n",
            "Natural         | compound   | Language\n",
            "Language        | compound   | Processing\n",
            "Processing      | nsubj      | is\n",
            "is              | ROOT       | is\n",
            "a               | det        | field\n",
            "fascinating     | amod       | field\n",
            "field           | attr       | is\n",
            "that            | nsubj      | combines\n",
            "combines        | relcl      | field\n",
            "linguistics     | dobj       | combines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîç 12. Text Similarity\n",
        "\n",
        "## üìå What is Text Similarity?\n",
        "\n",
        "**Text Similarity** measures how **similar or related two pieces of text are**. This is a fundamental task in NLP used in various applications such as:\n",
        "- üìë **Duplicate detection** (e.g., finding similar questions in forums)\n",
        "- üîé **Information retrieval** (e.g., ranking search results)\n",
        "- üí¨ **Chatbots and conversational agents**\n",
        "- üìÑ **Plagiarism detection**\n",
        "\n",
        "spaCy provides a simple way to compute **semantic similarity** between words, sentences, or entire documents using **word vectors** or **statistical models**.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "d5ISavWeMO7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_text_similarity(nlp):\n",
        "    \"\"\"Calculate similarity between texts\"\"\"\n",
        "    try:\n",
        "        # Two sample texts\n",
        "        doc1 = nlp(\"Apple is a technology company\")\n",
        "        doc2 = nlp(\"Microsoft is a software company\")\n",
        "\n",
        "        # Calculate similarity\n",
        "        similarity = doc1.similarity(doc2)\n",
        "\n",
        "        # Simple, clear output\n",
        "        print(\"=== TEXT SIMILARITY ===\")\n",
        "        print(f\"Text 1: Apple is a technology company\")\n",
        "        print(f\"Text 2: Microsoft is a software company\")\n",
        "        print(f\"Similarity: {similarity:.3f}\")\n",
        "\n",
        "        return similarity\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error: Need en_core_web_md model for similarity\")\n",
        "        print(\"Install with: python -m spacy download en_core_web_md\")\n",
        "        return None\n",
        "\n",
        "# Run the function\n",
        "similarity = spacy_text_similarity(nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqFIMDA6MOYa",
        "outputId": "5f2ad58a-fc68-4113-a89a-744269229849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TEXT SIMILARITY ===\n",
            "Text 1: Apple is a technology company\n",
            "Text 2: Microsoft is a software company\n",
            "Similarity: 0.917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-18-445675312.py:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity = doc1.similarity(doc2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj2eI2FpkPau"
      },
      "source": [
        "### Contributed by: Lama Ayash"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## References and Further Reading\n",
        "\n",
        "1. **Natural Language Toolkit (NLTK)**: https://www.nltk.org/\n",
        "   - Python library with extensive NLP tools and datasets\n",
        "\n",
        "2. **spaCy**: https://spacy.io/\n",
        "   - Industrial-strength NLP library\n",
        "\n",
        "3. **Hugging Face**: https://huggingface.co/\n",
        "   - Repository of pre-trained models and datasets\n",
        "\n",
        "4. **Stanford NLP Group**: https://nlp.stanford.edu/\n",
        "   - Research papers and resources\n"
      ],
      "metadata": {
        "id": "fav0r7yVc0oT"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}